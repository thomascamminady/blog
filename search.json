[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "It’s me, hi!",
    "section": "",
    "text": "It’s me, hi!\nMy name is Thomas, I’m a runner, mathematician, and currently working as a data scientist at Wahoo Fitness LLC. I studied Computational Engineering Science at RWTH Aachen University and then went ont to get a Ph.D. in applied mathematics from the Karlsruhe Institute of Technology.\n\n\n\n\n\nCan’t find a good image, ffs!\n\n\nWhen I’m not using pandas (or polars!) to turn wide dataframes into long ones, I like to run through forests and around tracks. I also like colors.\nReach out via this randomized, unique email address if you want to contact me!"
  },
  {
    "objectID": "contents/cv.html",
    "href": "contents/cv.html",
    "title": "Experience",
    "section": "",
    "text": "Experience\nWahoo Fitness, Atlanta/remote, Algorithm Developer\nSince March 2021\n\nCreation of APIs that allow the automatic monitoring of hardware and software quality metrics.\nTime series analysis and sensor fusion for various sources of data using Python.\nStatistical analysis of sports related data and creation of interactive dashboards for visualization.\nCode generation for low-level hardware using Matlab.\nCode documentation and result summaries in Confluence.\nSprint planning using Jira in an agile environment.\n\nSteinbuch Centre for Computing, Karlsruhe, Scientific Staff\nSeptember 2017 - March 2021\n\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nUsing concepts from machine learning and optimization theory, to automatically generate near-optimal sets of problem-dependent numerical parameters for the solution of advection problems.\nI optimized the performance of research software on the HPC cluster of the Karlsruhe Institute of Technology via parameter studies and OpenMP.\nFor modules in the mathematics M.Sc. program (including Kinetic Theory and Uncertainty Quantification) and modules in the Computational Engineering Science B.Sc. program, I was a teaching assistant and substituted lectures.\n\nCenter for Computational Engineering Science, Aachen, Scientific Staff\nMarch 2015 - September 2017\n\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nHelp with lectures in the Computational Engineering Module.\nPresentations at international conferences.\n\n\n\nEducation\nKarlsruhe Institute of Technology, Ph.D. Applied Mathematics\nSeptember 2017 to March 2021\n\nDissertation: “Theory, models, and numerical methods for classical and non-classical transport”\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nUsing concepts from machine learning and optimization theory, to automatically generate near- optimal sets of problem-dependent numerical parameters for the solution of advection problems.\nI optimized the performance of research software on the HPC cluster of the Karlsruhe Institute of Technology via parameter studies and OpenMP.\nFor modules in the mathematics M.Sc. program (including Kinetic Theory and Uncertainty Quantification) and modules in the Computational Engineering Science B.Sc. program, I was a teaching assistant and substituted lectures.\n\nRWTH Aachen University, M.Sc. Computational Engineering Science\nSeptember 2013 to September 2015\n\nThesis: “Theory and application of numerical methods for fractional diffusion equations”\nTutoring in the field of numerical mathematics and programming.\nResearch student at the Mathematical Center for Computational Engineering Science.\nCourses on (applied) mathematics, optimization methods, and algorithmic differentiation.\n\nRWTH Aachen University, B.Sc. Computational Engineering Science\nOctober 2009 to September 2013\n\nThesis: “Improvement of the aerodynamic shape optimization by adjoint methods in an MDO process”\nInternship and Bachelor’s thesis with EADS Cassidian, Manching, Germany.\nCourses on (applied) mathematics, fluid mechanics, and high-performance computing.\n\n\n\nPublications\nJournal articles\n\nMathematische Grundlagen der künstlichen Intelligenz im Schulunterricht, Sarah Schönbrodt, TC, and Martin Frank. Mathematische Semesterberichte, pages 1–29, 2021.\nRay Effect Mitigation for the Discrete Ordinates Method Using Artificial Scattering, Martin Frank, Jonas Kusch, TC, and Cory D. Hauck. Nuclear Science and Engineering, 194(11):971–988, 2020.\nRay effect mitigation for the discrete ordinates method through quadrature rotation, TC, Martin Frank, Kerstin Küpper, and Jonas Kusch. Journal of Computational Physics, 382:105 – 123, 2019.\nA spectral galerkin method for the fractional order diffusion and wave equation, TC and Martin Frank. International Journal of Advances in Engineering Sciences and Applied Mathematics, 10(1): 90–104, 2018.\nA new high-order fluid solver for tokamak edge plasma transport simulations based on a magnetic-field independent discretization, G. Giorgiani, TC, H. Bufferand, G. Ciraolo, P. Ghendrih, H. Guillard, H. Heumann, B. Nkonga, F. Schwander, E. Serre, and P. Tamain. Contributions to Plasma Physics, 58(6-8):688–695, 2018.\n\nConference proceedings\n\nHighly uniform quadrature sets for the discrete ordinates method, TC, Martin Frank, and Jonas Kusch. In Proc. Int. Conf. Mathematics and Computational Methods Applied to Nuclear Science and Engineering, pages 25–29, 2019.\nNonclassical particle transport in heterogeneous materials, TC, Martin Frank, and Edward W Larsen. In International Conference on Mathematics & Computational Methods Applied to Nuclear Science & Engineering, 2017.\nThe equivalence of forward and backward nonclassical particle transport theories, Edward W Larsen, Martin Frank, and TC. In International Conference on Mathematics & Computational Methods Applied to Nuclear Science & Engineering, 2017.\n\nBook chapters\n\nVorschlag für eine Abiturprüfungsaufgabe mit authentischem und relevantem Realitätsbezug, Maike Sube, TC, Martin Frank, and Christina Roeckerath. Springer Berlin Heidelberg, Berlin, Heidelberg, 2020."
  },
  {
    "objectID": "contents/posts/nutri-score/main.html",
    "href": "contents/posts/nutri-score/main.html",
    "title": "Nutriscore analysis",
    "section": "",
    "text": "Code\n# %load_ext autoreload\n# %autoreload 2\n\nimport os\nimport polars as pl\nimport altair as alt\nfrom camminapy.plot import altair_theme, altair_setup\nfrom rich import print\nfrom blog import logger\nfrom blog.io import (\n    FilterGermany,\n    ValidCode,\n    ConvertNutrientsToFloats,\n    HasNutriScore,\n    ConvertDatetimeStrings,\n    KeepOnlyEnglishVersion,\n    KeepOnlyTagsVersion,\n    ConvertNutriScoresToCapitalLetters,\n)\n\nalt.data_transformers.disable_max_rows()\naltair_theme()\n# altair_setup()\nlogger.setLevel(\"ERROR\")\n\nload_from_disk = True\nfinal_df_path = (\n    \"/Users/thomascamminady/Repos/blog/contents/posts/nutri-score/df.parquet\"\n)\nif load_from_disk and os.path.exists(final_df_path):\n    df = pl.read_parquet(final_df_path)\nelse:\n    path_csv = \"en.openfoodfacts.org.products.csv\"\n    path_parquet = path_csv.replace(\".csv\", \".parquet\")\n    if os.path.exists(path_parquet):\n        logger.info(\"Reading from .parquet file\")\n        _df = pl.read_parquet(\"en.openfoodfacts.org.products.parquet\")\n    else:\n        logger.info(\"Reading from .csv file.\")\n        _df = pl.read_csv(path_csv, separator=\"\\t\", ignore_errors=True)\n        _df.write_parquet(path_parquet)\n\n    df = (\n        _df.pipe(FilterGermany)  # We focus on products sold in Germany.\n        .pipe(HasNutriScore)  # We want products that have a non-null Nutriscore.\n        .pipe(ConvertNutrientsToFloats)  # Change dtype of columns with nutrients.\n        .pipe(ConvertDatetimeStrings)  # Change dtype to be Datetime.\n        .pipe(KeepOnlyEnglishVersion)  # Some duplicate columns, remove unwanted.\n        .pipe(KeepOnlyTagsVersion)  # Some duplicate columns, remove unwanted.\n        .pipe(ValidCode)  # Product code needs to be non-null and unique.\n        .pipe(ConvertNutriScoresToCapitalLetters)\n    )\n    df.write_parquet(final_df_path)\n\nn_non_nutrients = len([c for c in df.columns if not c.endswith(\"_100g\")])\nn_nutrients = len([c for c in df.columns if c.endswith(\"_100g\")])\nlogger.info(f\"\"\"Number of columns excluding nutrients: {n_non_nutrients}\"\"\")\nlogger.info(f\"\"\"Number of nutrients columns: {n_nutrients}\"\"\")\n\n\n\nThe Data\nThe data is taken from Open Food Facts where it is available for download under the Open Database License.\nI downloaded en.openfoodfacts.org.products.csv which is and 8.2GB file, containing 2.9 million products with 203 attributes stored per product, such as name, origin, sugar per 100g, etc.\nTo be able to work with this data, a couple of pre-processing steps and selections are executed which we will layout here with justifications if necessary:\n\nProducts must be sold in Germany. (I am German and I wanted to at least know some of the products. It also made my life easier because it reduces the amount of data by roughly a factor of 10.)\nProducts must have a valid Nutriscore. (The whole point of this analysis is to have a look at the Nutriscore, so items with a null Nutriscore are discarded. This reduces the data by another 3x.)\nRemove redundant information. (Some columns are redundant as they contain the same content, just in a different format. This drops about 25 columns.)\nRemove products with non-unique IDs. (This drops another couple of rows.)\n\nExecuting all these steps, we are left with around 73k products and 175 columns corresponding to different data fields. Out of those 175 columns, 118 contain information about the nutrients (per 100g).\nHere’s how the number of products split up across the different Nutriscores.\n\n\nCode\ndf.groupby(\"nutriscore_grade\", maintain_order=True).count().sort(\"nutriscore_grade\")\n\n\n\nshape: (5, 2)\n\n\n\nnutriscore_grade\ncount\n\n\nstr\nu32\n\n\n\n\n\"A\"\n11402\n\n\n\"B\"\n10033\n\n\n\"C\"\n16877\n\n\n\"D\"\n21253\n\n\n\"E\"\n13742\n\n\n\n\n\n\nIt is important to note, that products are listed in the data base multiple times, often because they occur in different portion sizes. For example, here are all products that are named “Snickers”.\n\n\nCode\ndf.filter(pl.col(\"product_name\") == \"Snickers\").select(\"product_name\", \"quantity\")\n\n\n\nshape: (8, 2)\n\n\n\nproduct_name\nquantity\n\n\nstr\nstr\n\n\n\n\n\"Snickers\"\n\"300 g, 6 barre…\n\n\n\"Snickers\"\n\"42g\"\n\n\n\"Snickers\"\n\"250g\"\n\n\n\"Snickers\"\n\"350 g (7 * 50 …\n\n\n\"Snickers\"\n\"275g\"\n\n\n\"Snickers\"\n\"50g\"\n\n\n\"Snickers\"\n\"49g\"\n\n\n\"Snickers\"\n\"275 g\"\n\n\n\n\n\n\n\n\nNutriscore 101\nThe Nutriscore shall serve as a tool that allows to quickly compare products. The score can be one of “A”, “B”, “C”, “D”, or “E”. However, there is also an actual numeric score underpinning the grade.\n(Note that while the official Nutriscore has a green-to-red scale, we are using a blue-to-red scale to ensure better readability for people with colorblindness.)\nThis is shown in the next graph which presents the histogram of these numeric scores, colored by their Nutriscore grade.\n\n\nCode\nc = \"nutriscore_grade\"\ncolor = (\n    alt.Color(f\"{c}:N\")\n    .scale(\n        zero=False,\n        domain=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n        # scheme=\"darkmulti\",\n        range=[\"blue\", \"lightblue\", \"gold\", \"orange\", \"red\"],\n    )\n    .legend(columns=1, symbolLimit=0, labelLimit=0)\n)\n\nalt.Chart(\n    df.select(\"nutriscore_grade\", \"nutriscore_score\")  # .sample(5_000)\n).mark_bar().encode(\n    x=alt.X(\"nutriscore_score:Q\").bin(step=1),\n    y=alt.Y(\"count():Q\"),\n    color=color,\n    row=\"nutriscore_grade:N\",\n).properties(\n    height=150, width=700\n).resolve_scale(\n    y=\"independent\"\n)\n\n\n\n\n\n\n\n\nThis mostly agrees with what I could find online\n\n\n\nNutriscore.jpeg\n\n\nInterestingly, there are a number of products that have an “E” grade but a lower numeric Nutriscore. Looking at a random selection of those, it seems that these are mostly beverages containing sugar.\n\n\nCode\nprint(\n    df.filter(pl.col(\"nutriscore_grade\") == \"E\")\n    .filter(pl.col(\"nutriscore_score\") &lt; 18)\n    .select(\"product_name\", \"categories_en\")\n    .sort(\"categories_en\")\n    .sample(n=20)[\"product_name\"]\n    .to_list()\n)\n\n\n[\n    'Rockstar xdurance energy grape',\n    'BraTee Wassermelone',\n    'Doppelkaramel',\n    'Uludag ORANGE',\n    'Apfelsaft',\n    'Erdbeer Banane Traube Smoothie',\n    'Vielanker Fassbrause Blaubeere',\n    'Zitronenlimonade',\n    'Apfel-Direktsaft naturtrüb',\n    'Orange-Mango Kurkuma & Weisser Tee-Essenz',\n    'Getränke - milder MULTI',\n    'Coca-Cola Classic',\n    'Coca-Cola',\n    'Happy day 100% Orange mild',\n    'Misch Masch',\n    'Cocktail limo',\n    'korrekt Eistee Zitrone',\n    'Ayran',\n    'Hohes C PLUS Eisen',\n    'Grapefruit Getränk'\n]\n\n\n\nNext, we can have a look at some of the nutrients. Let’s plot fat per 100g against sugar per 100g and group everything by Nutriscore.\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n\n\nchart = (\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").is_null().is_not()).select(x, y, c)\n        # .sample(5_000)\n    )\n    .mark_point(clip=True, filled=True, opacity=0.2)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n        tooltip=[\"product_name:N\"],\n    )\n    .properties(width=220, height=300)\n    .facet(facet=f\"{c}:N\", columns=3)\n)\n\nchart\n\n\n\n\n\n\n\n\nUnsuprisingly, “D” and “E” grade products seem to contain more sugar and fat.\nLet’s look at a histogram of the contained calories (per 100g) next.\n\n\nCode\nalt.Chart(\n    df.select(\"nutriscore_grade\", \"nutriscore_score\", \"energy-kcal_100g\").filter(\n        pl.col(\"energy-kcal_100g\") &lt; 1000\n    )  # .sample(5_000)\n).mark_bar().encode(\n    x=alt.X(\"energy-kcal_100g:Q\").bin(step=10),\n    y=alt.Y(\"count():Q\"),\n    color=color,\n    row=\"nutriscore_grade:N\",\n).properties(\n    height=150, width=700\n).resolve_scale(\n    y=\"independent\"\n)\n\n\n\n\n\n\n\n\nInterestingly, there is a spike at around 300kcal for the “A” grade foods, let’s look at what they are.\n\n\nCode\ndf.filter(pl.col(\"nutriscore_grade\") == \"A\").filter(\n    pl.col(\"energy-kcal_100g\").is_between(300, 350)\n).select(\"product_name\", \"categories_en\").sample(20)[\"product_name\"].to_list()\n\n\n['Weizenmehl Type 1050',\n 'mehl Farine type 00',\n 'Hesperiden Essig',\n 'Weizenmehl Type 405',\n 'Hartweizen Grieß',\n 'Beluga Linsen',\n 'Beans, Baby Lima - dry',\n 'Nudeln Dinkel Vollkorn Spaghetti',\n 'Roggenmehl Type 960',\n 'Fusilli',\n 'Rote Linsen Risoni',\n 'Buchweizenmehl',\n 'Pizza Mehl',\n 'Roggenmehl T 1150',\n 'VK Buchweizenmehl Dose',\n 'Spitzen Langkorn Reis',\n 'Sarrasin Bio',\n 'polenta',\n 'Spezialmehl für Pizza',\n 'Früchte Müsli Vollkorn']\n\n\nThis is containing a lot of pasta, cereal and rice, but also flour.\nLet’s have a look at the top ten healthiest foods, according to their Nutriscore.\n\n\nCode\ndf.sort(\"nutriscore_score\").head(10)[\"product_name\"].to_list()\n\n\n['Veggy Love Mexican',\n 'Schwarze Bohnen-Tempeh Natur',\n 'Wachtelbohnen-Tempeh Natur',\n 'junge Erbsen',\n 'Morchel-Hüte',\n 'Veggie Love Orient',\n 'Kapucijners',\n 'Kichererbsen Bohnen-Tempeh Natur',\n 'Gemüse Erbsen',\n 'Knoblauch granuliert']\n\n\nAnd of course the ten least healthy foods.\n\n\nCode\ndf.sort(\"nutriscore_score\").tail(10)[\"product_name\"].to_list()\n\n\n['Weiße Schokolade',\n 'Chai latte Classic India - Typ Vanille-Zimt',\n 'Schweizer Bio-Alpenvollmilchschokolade',\n 'chai latte',\n 'Chocolat Bourbon Vanille',\n 'chai latte',\n 'Zarte weiße Schokolade',\n 'Bio-Trinkschokolade',\n 'Flap Jack Apricot Smoothie',\n 'Chai Latte, Classic India Weniger Süss Typ Vanille...']\n\n\nThis also makes sense I guess. So far so good.\n\n\nGreenwashing?\nVery frequently I can’t believe how a specific product got an “A” grade, like how does some chocolate get an “A” grade even though it is high of sugar. Is there any way that companies are cheating the system?\nLet’s have a look at all the products that contain the word “choco” and see if we find something.\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").str.to_lowercase().str.contains(\"choco\"))\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=300, height=300)\n)\n\n\n\n\n\n\n\n\nHmm so this seems to suggest that “A” grades are actually properly assigned when sugar and fat content is low.\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").str.to_lowercase().str.contains(\"yoghurt\"))\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=300, height=300)\n)\n\n\n\n\n\n\n\n\nHow about “sweet snacks” (a label in the data frame) in general?\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(\n            pl.col(\"categories_en\").str.to_lowercase().str.contains(\"sweet snacks\")\n        )\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True, opacity=0.2)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=300, height=300)\n)\n\n\n\n\n\n\n\n\nWhat about vegan labels?\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(\n            pl.col(\"ingredients_analysis_tags\").str.to_lowercase().str.contains(\"vegan\")\n        )\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=200, height=300)\n    .facet(facet=\"nutriscore_grade:N\", columns=3)\n)\n\n\n\n\n\n\n\n\nNext is protein vs. sugar.\n\n\nCode\nx = \"sugars_100g\"\ny = \"proteins_100g\"\n(\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").str.to_lowercase().str.contains(\"choco\"))\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=200, height=300)\n    .facet(facet=\"nutriscore_grade:N\", columns=3)\n)\n\n\n\n\n\n\n\n\nSo no greenwashing? Need to continue this.\n\n\nCorrelations\n\n\nCode\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\nimport collections\nimport numpy as np\n\n\nLet’s see which of the nutrients correlate with the Nutriscore.\nTo do that, we pick all columns that contain ingredients about the nutrients and where at most 10% of the entries are null.\n\n\nCode\nd = (\n    (\n        df.select([c for c in df.columns if c.endswith(\"_100g\")]).null_count()\n        / len(df)\n        * 100\n    )\n    &lt;= 10\n)[0].to_dict()\ncolumns = [\"nutriscore_score\"] + [key for key, values in d.items() if values[0]]\n\ndisplay(\n    df.select(columns)\n    .to_pandas()\n    .corr()[[\"nutriscore_score\"]]\n    .iloc[1:-1, :]\n    .sort_values(\"nutriscore_score\")\n    .style.background_gradient(cmap=\"RdBu\", vmin=-1, vmax=1)\n)\n\n\n\n\n\n\n\n \nnutriscore_score\n\n\n\n\nproteins_100g\n0.077772\n\n\nsalt_100g\n0.094111\n\n\nsodium_100g\n0.094118\n\n\ncarbohydrates_100g\n0.146849\n\n\nenergy-kcal_100g\n0.222867\n\n\nenergy_100g\n0.336517\n\n\nsugars_100g\n0.370191\n\n\nfat_100g\n0.488456\n\n\nsaturated-fat_100g\n0.578473\n\n\n\n\n\nSo (saturated) fat is the best indicator for a Nutriscore.\nThe obvious next question is, how well can we predict this. We’ll use simple Decision Trees.\n\n\nCode\ncolumns_for_fitting = [\n    \"energy-kcal_100g\",\n    \"energy_100g\",\n    \"fat_100g\",\n    \"saturated-fat_100g\",\n    \"carbohydrates_100g\",\n    \"sugars_100g\",\n    \"proteins_100g\",\n    \"salt_100g\",\n    \"sodium_100g\",\n    # \"nutrition-score-fr_100g\",\n]\n\n# columns_for_fitting =columns\n\n\n\n\nCode\ny = df.select(\"nutriscore_score\").to_numpy().flatten()\nlogger.info(y.shape)\n\nX = df.select(\n    [\n        c\n        for c in columns_for_fitting\n        if (c != \"nutriscore_score\" and c != \"nutrition-score-fr_100g\")\n    ]\n).to_numpy()\nlogger.info(X.shape)\n\n\nlogger.info(\"Keep only data that has no nans.\")\nimport numpy as np\n\nselect = np.sum(np.isnan(X), axis=1) == 0\ny = y[select]\nX = X[select, :]\nlogger.info(y.shape)\nlogger.info(X.shape)\n\n\ntransformer = Normalizer().fit(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    transformer.transform(X), y, test_size=0.20, random_state=2023\n)\n\n\nclf = tree.DecisionTreeRegressor(max_depth=15)\nclf = clf.fit(X_train, y_train)\n\n\ndf_tree = pl.concat(\n    [\n        pl.DataFrame(\n            {\n                \"actual score\": y_test,\n                \"predicted score\": clf.predict(X_test),\n                \"label\": \"test\",\n            }\n        ),\n        pl.DataFrame(\n            {\n                \"actual score\": y_train,\n                \"predicted score\": clf.predict(X_train),\n                \"label\": \"train\",\n            }\n        ),\n    ]\n).with_columns(err=pl.col(\"predicted score\") - pl.col(\"actual score\"))\n\n\nchart_v1 = (\n    alt.Chart(df_tree)\n    .mark_point(filled=True, opacity=0.02)\n    .encode(\n        x=\"actual score:Q\",\n        y=\"predicted score:Q\",\n        # y=\"err:Q\",\n        color=\"label:N\",\n        column=\"label:N\",\n    )\n    .properties(width=300, height=300)\n)\n\ndisplay(chart_v1)\n\n\n\n\n\n\n\n\nA little underwhelming. Maybe adding information on the categories helps. Let’s find the top 20 labels and one-hot-encode them.\n\n\nCode\nlol_categories = (\n    df.select(\"categories_en\")\n    .with_columns(pl.col(\"categories_en\").str.split(\",\"))[\"categories_en\"]\n    .to_list()\n)\n\n\ncategories = []\nfor l in lol_categories[:100]:\n    categories.extend(l)\n\n\nhist = dict(collections.Counter(categories))\n\ndf_hist = (\n    pl.DataFrame(\n        {\n            \"categorie\": [key for key, _ in hist.items()],\n            \"count\": [value for _, value in hist.items()],\n        }\n    )\n    .sort(\"count\", descending=True)\n    .head(20)\n)\nprint(df_hist[\"categorie\"].to_list())\n\n\n\n[\n    'Plant-based foods and beverages',\n    'Plant-based foods',\n    'Beverages',\n    'Cereals and potatoes',\n    'Snacks',\n    'Cereals and their products',\n    'Sweet snacks',\n    'Cocoa and its products',\n    'Carbonated drinks',\n    'Sodas',\n    'Chocolates',\n    'Colas',\n    'Canned foods',\n    'Meals',\n    'Fruits and vegetables based foods',\n    'Artificially sweetened beverages',\n    'Sweetened beverages',\n    'Seeds',\n    'Cereal grains',\n    'Canned plant-based foods'\n]\n\n\n\nAlright so we can now one-hot-encode these. Let’s run the classifier again, but now we add these labels. We can also check the correlation again.\n\n\nCode\ndf_one_hot = df.with_columns(\n    (pl.col(\"categories_en\").str.to_lowercase().str.contains(x.lower()))\n    .cast(int)\n    .alias(f\"1hot__{x}\")\n    for x in df_hist[\"categorie\"].to_list()\n)\ncolumns_one_hot = [c for c in df_one_hot.columns if \"1hot\" in c]\nlogger.info(columns_one_hot)\n\ny = df_one_hot.select(\"nutriscore_score\").to_numpy().flatten()\nlogger.info(y.shape)\n\nX = df_one_hot.select(\n    [\n        c\n        for c in columns_for_fitting + columns_one_hot\n        if (c != \"nutriscore_score\" and c != \"nutrition-score-fr_100g\")\n    ]\n).to_numpy()\nlogger.info(X.shape)\n\n\nlogger.info(\"Keep only data that has no nans.\")\n\nselect = np.sum(np.isnan(X), axis=1) == 0\ny = y[select]\nX = X[select, :]\nlogger.info(y.shape)\nlogger.info(X.shape)\n\nd = (\n    (\n        df_one_hot.select(\n            [c for c in df_one_hot.columns if c.endswith(\"_100g\")]\n        ).null_count()\n        / len(df)\n        * 100\n    )\n    &lt;= 10\n)[0].to_dict()\n\ncolumns = (\n    [\"nutriscore_score\"]\n    + [key for key, values in d.items() if values[0]]\n    + columns_one_hot\n)\n\ndisplay(\n    df_one_hot.select(columns)\n    .to_pandas()\n    .corr()[[\"nutriscore_score\"]]\n    .iloc[1:-1, :]\n    .sort_values(\"nutriscore_score\")\n    .style.background_gradient(cmap=\"RdBu\", vmin=-1, vmax=1)\n)\n\n\nlogger.info(X.shape)\ntransformer = Normalizer().fit(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    transformer.transform(X), y, test_size=0.20, random_state=2023\n)\n\nclf = tree.DecisionTreeRegressor(max_depth=15)\nclf = clf.fit(X_train, y_train)\n\n\ndf_tree_one_hot = pl.concat(\n    [\n        pl.DataFrame(\n            {\n                \"actual score\": y_test,\n                \"predicted score\": clf.predict(X_test),\n                \"label\": \"test\",\n            }\n        ),\n        pl.DataFrame(\n            {\n                \"actual score\": y_train,\n                \"predicted score\": clf.predict(X_train),\n                \"label\": \"train\",\n            }\n        ),\n    ]\n).with_columns(err=pl.col(\"predicted score\") - pl.col(\"actual score\"))\n\n\nnew_chart = (\n    alt.Chart(df_tree_one_hot)\n    .mark_point(filled=True, opacity=0.02)\n    .encode(\n        x=\"actual score:Q\",\n        y=\"predicted score:Q\",\n        # y=\"err:Q\",\n        color=\"label:N\",\n        column=\"label:N\",\n    )\n    .properties(width=300, height=300)\n)\n\n\nchart_v1 & new_chart\n\n\n\n\n\n\n\n \nnutriscore_score\n\n\n\n\n1hot__Beverages\n-0.393359\n\n\n1hot__Plant-based foods\n-0.386696\n\n\n1hot__Plant-based foods and beverages\n-0.386696\n\n\n1hot__Cereals and potatoes\n-0.274596\n\n\n1hot__Cereals and their products\n-0.267379\n\n\n1hot__Fruits and vegetables based foods\n-0.224242\n\n\n1hot__Seeds\n-0.180736\n\n\n1hot__Canned foods\n-0.158242\n\n\n1hot__Cereal grains\n-0.135658\n\n\n1hot__Meals\n-0.105492\n\n\n1hot__Carbonated drinks\n-0.046306\n\n\n1hot__Artificially sweetened beverages\n-0.041858\n\n\n1hot__Sweetened beverages\n-0.017318\n\n\n1hot__Colas\n-0.008903\n\n\n1hot__Sodas\n-0.005889\n\n\nproteins_100g\n0.077772\n\n\nsalt_100g\n0.094111\n\n\nsodium_100g\n0.094118\n\n\ncarbohydrates_100g\n0.146849\n\n\nenergy-kcal_100g\n0.222867\n\n\n1hot__Chocolates\n0.308169\n\n\nenergy_100g\n0.336517\n\n\nsugars_100g\n0.370191\n\n\n1hot__Cocoa and its products\n0.387590\n\n\n1hot__Snacks\n0.444509\n\n\n1hot__Sweet snacks\n0.450395\n\n\nfat_100g\n0.488456\n\n\nsaturated-fat_100g\n0.578473\n\n\nnutrition-score-fr_100g\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_tree_full = pl.concat(\n    [\n        df_tree.with_columns(pl.lit(\"simple\").alias(\"model\")),\n        df_tree_one_hot.with_columns(pl.lit(\"one_hot\").alias(\"model\")),\n    ]\n)\n\n\n\n\nCode\nalt.Chart(df_tree_full).mark_bar().encode(\n    x=alt.X(\"err:Q\").bin(step=2).axis(values=np.arange(-30, 35, 5)),\n    y=alt.Y(\"count():Q\"),\n    color=\"label:N\",\n    row=\"model:N\",\n    column=\"label:N\",\n).properties(width=300, height=200).resolve_scale(y=\"independent\")\n\n\n\n\n\n\n\n\n\n\nCode\ndf_tree_full.groupby(\"label\", \"model\").agg(\n    err_min=pl.col(\"err\").min(),\n    err_mean=pl.col(\"err\").mean(),\n    err_median=pl.col(\"err\").median(),\n    err_std=pl.col(\"err\").std(),\n    err_max=pl.col(\"err\").max(),\n)\n\n\n\nshape: (4, 7)\n\n\n\nlabel\nmodel\nerr_min\nerr_mean\nerr_median\nerr_std\nerr_max\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"train\"\n\"one_hot\"\n-21.017073\n-3.5023e-17\n0.0\n2.195532\n18.509434\n\n\n\"train\"\n\"simple\"\n-25.449679\n1.2140e-16\n0.0\n2.757782\n18.552941\n\n\n\"test\"\n\"one_hot\"\n-30.0\n-0.017979\n0.0\n3.950445\n25.0\n\n\n\"test\"\n\"simple\"\n-29.0\n-0.058111\n0.0\n4.967512\n36.0\n\n\n\n\n\n\nThis looks fine for now, with the one-hot-encoded model, the error is mostly acceptable.\nHowever, let’s try one more time, this time with classification instead of regression.\n\n\nCode\ndf_one_hot = df.with_columns(\n    (pl.col(\"categories_en\").str.to_lowercase().str.contains(x.lower()))\n    .cast(int)\n    .alias(f\"1hot__{x}\")\n    for x in df_hist[\"categorie\"].to_list()\n)\ncolumns_one_hot = [c for c in df_one_hot.columns if \"1hot\" in c]\n\ny = df_one_hot.select(\"nutriscore_grade\").to_numpy().flatten()\n\nX = df_one_hot.select(\n    [\n        c\n        for c in columns_for_fitting + columns_one_hot\n        if (c != \"nutriscore_score\" and c != \"nutrition-score-fr_100g\")\n    ]\n).to_numpy()\n\n\nselect = np.sum(np.isnan(X), axis=1) == 0\ny = y[select]\nX = X[select, :]\nlogger.info(y.shape)\nlogger.info(X.shape)\n\nfrom sklearn.metrics import confusion_matrix\n\n\nclf = tree.DecisionTreeClassifier(max_depth=10)\ntransformer = Normalizer().fit(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    transformer.transform(X), y, test_size=0.80, random_state=2023\n)\nclf.fit(X_train, y_train)\n\ny_test_predict = clf.predict(X_test)\ny_train_predict = clf.predict(X_train)\n\nlogger.info(\"Training confusion matrix\")\n# cm = confusion_matrix(y_test, y_test_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\ncm = confusion_matrix(y_train, y_train_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\"\"\"Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\"\"\"\nletter = {4: \"A\", 3: \"B\", 2: \"C\", 1: \"D\", 0: \"E\"}\ndisplay(\n    pl.DataFrame(cm)\n    .rename(\n        mapping={\n            f\"column_{i}\": f\"predicts {letter}\"\n            for i, letter in zip([0, 1, 2, 3, 4], [\"A\", \"B\", \"C\", \"D\", \"E\"])\n        }\n    )\n    .with_columns(\n        pl.col(\"predicts A\")\n        .rank()\n        .cast(int)\n        .apply(lambda r: letter[r - 1])\n        .alias(\"true label\")\n    )\n    .select(\n        \"true label\",\n        \"predicts A\",\n        \"predicts B\",\n        \"predicts C\",\n        \"predicts D\",\n        \"predicts E\",\n    )\n    .to_pandas()\n    .style.background_gradient()\n)\n\n\nlogger.info(\"Testing confusion matrix\")\ncm = confusion_matrix(y_test, y_test_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n# cm = confusion_matrix(y_train, y_train_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\"\"\"Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\"\"\"\nletter = {4: \"A\", 3: \"B\", 2: \"C\", 1: \"D\", 0: \"E\"}\ndisplay(\n    pl.DataFrame(cm)\n    .rename(\n        mapping={\n            f\"column_{i}\": f\"predicts {letter}\"\n            for i, letter in zip([0, 1, 2, 3, 4], [\"A\", \"B\", \"C\", \"D\", \"E\"])\n        }\n    )\n    .with_columns(\n        pl.col(\"predicts A\")\n        .rank()\n        .cast(int)\n        .apply(lambda r: letter[r - 1])\n        .alias(\"true label\")\n    )\n    .select(\n        \"true label\",\n        \"predicts A\",\n        \"predicts B\",\n        \"predicts C\",\n        \"predicts D\",\n        \"predicts E\",\n    )\n    .to_pandas()\n    .style.background_gradient()\n)\n\n\n\n\n\n\n\n \ntrue label\npredicts A\npredicts B\npredicts C\npredicts D\npredicts E\n\n\n\n\n0\nA\n1678\n123\n216\n33\n2\n\n\n1\nB\n274\n975\n433\n143\n13\n\n\n2\nC\n89\n179\n2260\n690\n29\n\n\n3\nD\n14\n41\n373\n3389\n123\n\n\n4\nE\n4\n12\n153\n347\n1981\n\n\n\n\n\n\n\n\n\n\n \ntrue label\npredicts A\npredicts B\npredicts C\npredicts D\npredicts E\n\n\n\n\n0\nA\n5806\n852\n1087\n250\n26\n\n\n1\nB\n1427\n3324\n2121\n651\n54\n\n\n2\nC\n491\n1092\n7421\n3401\n236\n\n\n3\nD\n140\n320\n2188\n12216\n1024\n\n\n4\nE\n45\n106\n684\n1886\n7448\n\n\n\n\n\n\n\nSummary\nThis was a good first deep dive into the data. Nothing suspicous as of yet. No obvious green washing and prediction works reasonably well (as expected). To be continued.!"
  },
  {
    "objectID": "contents/posts/world-records/records.html",
    "href": "contents/posts/world-records/records.html",
    "title": "Track and Field World Record Progression",
    "section": "",
    "text": "The figure below show how the track and field world records have progressed over time.\nClick on the legend to highlight individual events and see the respective world record holders.\n\n\nCode\nimport altair as alt\nimport polars as pl\nfrom alltime_athletics_python.io import download_data\nfrom alltime_athletics_python.io import import_running_only_events\n\n# download_data()\ndf = import_running_only_events(\"./data\")\n\n\n\n\nCode\nworld_records = (\n    df.filter(pl.col(\"event\").str.contains(\"walk\") == False)\n    .filter(pl.col(\"event type\") == \"standard\")\n    .sort(\"sex\", \"distance\", \"event\", \"date of event\")\n    .with_columns(\n        pl.col(\"result seconds\")\n        .cummin()\n        .over(\"sex\", \"event\")\n        .alias(\"world record time\")\n    )\n    .filter(pl.col(\"result seconds\") == pl.col(\"world record time\"))\n    .groupby(\"sex\", \"event\", \"result seconds\", maintain_order=True)\n    .first()\n    .with_columns(\n        (\n            100\n            * pl.col(\"result seconds\")\n            / pl.col(\"result seconds\").min().over(\"sex\", \"event\")\n        ).alias(\"percent of wr\")\n    )\n)\n\nworld_records = pl.concat(\n    [\n        world_records,\n        world_records.filter(pl.col(\"rank\") == 1).with_columns(\n            [\n                pl.lit(\"2023-06-07\")\n                .str.strptime(pl.Date, format=\"%Y-%m-%d\")\n                .alias(\"date of event\"),\n                pl.lit(-1).cast(pl.Int64).alias(\"rank\"),\n            ]\n        ),\n    ]\n).with_columns(pl.col(\"sex\").apply(lambda s: s.title()))\n\ndata = world_records.select(\n    \"date of event\", \"percent of wr\", \"event\", \"sex\", \"name\", \"rank\"\n).to_pandas()\nlegend_selection = alt.selection_point(fields=[\"event\"], bind=\"legend\")\nlegend_selection_empty = alt.selection_point(\n    fields=[\"event\"], bind=\"legend\", empty=False\n)\n\nbase = (\n    alt.Chart(data)\n    .encode(\n        x=alt.X(\"date of event:T\")\n        .scale(domain=(\"1950-01-01\", \"2026-01-01\"))\n        .title(\"Year\"),\n        y=alt.Y(\"percent of wr:Q\")\n        .scale(domain=(100, 110))\n        .axis(values=list(range(100, 120, 2)))\n        .title(\"Time in % of current WR\"),\n        color=alt.Color(\n            \"event:N\",\n            sort=world_records.sort(\"distance\")[\"event\"]\n            .unique(maintain_order=True)\n            .to_list(),\n        )\n        .scale(scheme=\"dark2\")\n        .title(\"Event\"),\n        # strokeDash=\"sex:N\",\n        opacity=alt.condition(legend_selection, alt.value(1), alt.value(0)),\n    )\n    .properties(width=800, height=500)\n    .add_params(legend_selection)\n    .add_params(legend_selection_empty)\n)\n\nbase_no_endpoint = base.transform_filter(alt.datum[\"rank\"] &gt; 0)\n\ntext = base_no_endpoint.encode(\n    text=\"name:N\",\n    opacity=alt.condition(legend_selection_empty, alt.value(0.9), alt.value(0.0)),\n)\n\n\nchart = (\n    alt.layer(\n        base.mark_line(interpolate=\"step-after\", clip=True, strokeWidth=3),\n        base_no_endpoint.mark_point(filled=True, clip=True, size=100),\n        text.mark_text(clip=True, fontSize=14, angle=270 + 45, align=\"left\", dx=15),\n    )\n    .facet(\n        row=alt.Row(\"sex:N\").title(\"\").header(labelAngle=0),\n        title=\"World Record Progression\",\n    )\n    .resolve_scale(x=\"independent\")\n)\n\ndisplay(chart)"
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html",
    "href": "contents/posts/logitech-issue/index.html",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "",
    "text": "About two month ago, I bought a Logitech MX Mechanical Mini for Mac and it worked like a charm with my previous Intel Macbook Pro (2019). Last week I received a new work Macbook that uses the newer M1 chip (M1 Max). Since then I am experiencing issues with the connectivity of the MX Mechanical Mini.\nAs of now (Feb 22, 2023) I have no solution for this issue."
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html#whats-the-issue",
    "href": "contents/posts/logitech-issue/index.html#whats-the-issue",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "What’s the issue?",
    "text": "What’s the issue?\nIt’s a connectivity issue, where it seems like the keyboard goes to sleep after only a couple of seconds of inactivity (smaller than ten seconds). Waking up the keyboard then takes some three to four seconds, time during which no keystrokes are recorded. This is especially annoying for me, since I work as a developer and there are frequent periods of not typing (but thinking), followed by quick bursts of typing.\nI did not notice this issue on the Intel-based Macbook which makes me wonder whether this is an issue on the hardware side of things."
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html#what-have-i-tried",
    "href": "contents/posts/logitech-issue/index.html#what-have-i-tried",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "What have I tried?",
    "text": "What have I tried?\nI tried the usual things: - Making sure that the battery is charged. - Updating driver (Logi Options+) - Disconnect the device, forget the device in bluetooth settings, and reconnect. - Factory reset the keyboard, including the removal of all previously paired devices. - Pairing my Mac on each of the 3 different Easy-Switch slots to see whether the slot makes a difference."
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html#references-ressources",
    "href": "contents/posts/logitech-issue/index.html#references-ressources",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "References / Ressources",
    "text": "References / Ressources\nI looked for this issue and found some recent posts / threads on reddit: - MX mechanical sleeps - MX keys randomly disconnecting\nThis website explains how to factory reset your keyboard such that all previously paired devices are forgotten: - Factory reset Logitech MX Keys Keyboard (Note: The sequence is esc O esc O esc B, where O is the fifteenth letter of the alphabet, not the number 0.)"
  },
  {
    "objectID": "contents/posts/gradeinflation/Abiturnoten.html",
    "href": "contents/posts/gradeinflation/Abiturnoten.html",
    "title": "Abiturnoten inflation in Germany",
    "section": "",
    "text": "Code\nfrom create_plot import run\n\nrun()\n\n\nThe German Abitur - colloquially known as the ‘Abitur’ - represents the culmination of a student’s secondary education. This advanced level of high school education concludes with an exam of significant importance. Successfully passing the Abitur opens the door to tertiary education institutions - universities, colleges, and certain professional schools. Given the weightage of this examination, an upward trend in the grades observed over the years deserves a closer inspection.\nRecently, an intriguing analysis was conducted, plotting the trajectory of Abitur grades over time. The resulting graph reveals a surprising trend: while there has been an increase in the number of good grades, the number of mediocre grades has largely remained unchanged, and bad grades have seen a decline. This pattern merits a deeper dive into the phenomenon of grade inflation, a subject that has been a topic of substantial debate in educational circles worldwide.\nGrade inflation refers to the trend where higher grades are awarded for the same quality of work compared to previous years. In essence, the standard of what constitutes a ‘good’ or ‘excellent’ grade is slowly diminished. This can lead to a devaluation of educational attainment, making it more challenging for universities and employers to discern the calibre of graduates. It also raises concerns about the efficacy of the education system itself: are students truly performing better, or are the standards being lowered?\nThe figure produced from the recent analysis offers visual evidence of grade inflation in the Abitur. The rising trend of good grades suggests that more and more students are performing at a level previously considered exceptional. On the surface, this could be interpreted as a positive development, signaling increased student achievement. However, the stagnation of mediocre grades and the decrease in poor grades provide a different perspective.\nWhy are mediocre grades stagnating, and why are bad grades dwindling? If the education system is improving uniformly, we would expect an overall shift with improvements across the board. The specificity of this trend suggests that the bar for what constitutes good performance might be lowering, thereby allowing more students to achieve ‘good’ grades.\nFurthermore, this pattern is potentially problematic for universities and employers who rely on these grades to gauge a student’s competence. If more students are receiving good grades, the differentiation between an average and an outstanding student becomes blurred, reducing the efficacy of grades as a tool for assessing student abilities.\nThis analysis is crucial in sparking further discussion on the phenomenon of grade inflation. It opens up a much-needed discourse on the current educational standards and how they are evolving. It also brings to light the importance of maintaining rigorous, consistent grading standards to ensure that grades remain a reliable measure of student achievement.\nThe journey to understand the German Abitur’s evolving landscape continues, and such investigations provide essential insights. They remind us that while grades are critical, they are just one aspect of education. The ultimate goal should always be the cultivation of knowledge, skills, and character, ensuring that students are truly prepared for the challenges they will face in their futures.\n\n\n\nGrade inflation German Abitur, share of 1.0.\n\n\n\n\n\nGrade inflation German Abitur"
  },
  {
    "objectID": "contents/posts/verkehr/verkehr.html",
    "href": "contents/posts/verkehr/verkehr.html",
    "title": "Das Verkehrsproblem in Balve lösen",
    "section": "",
    "text": "Einfache und günstige Maßnahmen können die Lebensqualität der Bürger:innen drastisch verbessern.\n\nDas Problem\nNicht nur seit der Schließung der Autobahnbrücke gibt es folgende Probleme:\n\nEin enorm starkes Verkehrsaufkommen. Das Kreuzen der Hauptstraße im Stadtzentrum ist de facto nicht sicher möglich.\nZu viele LKW quetschen sich durch die Balver Hauptstraße. Neben dem Sicherheitsaspekt kommt hier auch ein erhöhtes Lärmaufkommen und Unfallgefahr hinzu. Erholung im Stadtgebiet ist nicht möglich.\nKeine Radwege oder Fahrradschutzstreifen entlang der Hauptstraße. Stadtradeln ist zwar eine schöne Idee, sicher fühlt man sich aber nicht.\nEs wird zu häufig zu schnell gefahren. Insbesondere mit parkenden Autos ist Tempo 50 zu hoch.\n\n\n\nEine Lösung\nDie Probleme lassen sich einfach entschärfen. Fahren im Stadtgebiet Balve kann sicherer gemacht werden, indem Tempo 30 eingeführt wird. Zudem verhindert ein LKW-Verbot zwischen Langenholthausen und Balve, dass sich schweres Gefährt durch die Stadtmitte quetscht. Der Verkehr kann problemlos umgeleitet werden. Dies ist in der folgenden Grafik illustriert. Fahrradschutzstreifen machen es für Autofahrer:innen deutlicher, dass nur bei ausreichendem Abstand überholt werden darf. Zudem ist ein Zebrastreifen auf Höhe der Volksbank notwendig um das Überqueren der Hauptstraße wieder sicher zu machen.\n\n\n\nEine mögliche Umleitung\n\n\n\n\nDie Ausreden\nHier soll nur kurz auf mögliche Argumente eingegangen werden.\n\nNicht genügend Platz für Autos.\nFahrräder müssten laut StVO auch jetzt schon innerorts mit 1.5m Abstand überholt werden. Ein Schutzstreifen für Radfahrende würde also nichts an der Platzverteilung im Straßenverkehr ändern.\n\n\nDas ist zu teuer!\nDie Kosten würden sich auf ein paar wenige Schilder und Farbe belaufen.\n\n\nDas liegt in der Hand von Straßen.NRW und wir können da nichts machen.\nSich für die Belange der Bürger:innen einzusetzen ist genau die Aufgabe der lokalen Politiker:innen. Sicherlich lässt sich hier ein Weg finden; andere Städte im näheren Umkreis haben es auch geschafft."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html",
    "href": "contents/posts/polars-vs-pandas/index.html",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "",
    "text": "A couple of weeks ago I came across polars, a “Lightning-fast DataFrame library for Rust and Python.” Since then, I have been playing around with it, trying to do some of my daily data analyses tasks with polars instead of pandas.\nI wanted to summarize my experience using polars for some of the work that I am doing by comparing my polars implementation of a data analysis pipeline to the equivalent pipeline using pandas. The emphasis here is on the fact that it is my implementation. I am sure that both the polars and the pandas implementation can be improved or are not necessarily following best practices. Moreover, I am barely fluent in polars at this point.\nNevertheless, I think that I learned something for myself and have formed some opinion on things I like and dislike."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#preface",
    "href": "contents/posts/polars-vs-pandas/index.html#preface",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "",
    "text": "A couple of weeks ago I came across polars, a “Lightning-fast DataFrame library for Rust and Python.” Since then, I have been playing around with it, trying to do some of my daily data analyses tasks with polars instead of pandas.\nI wanted to summarize my experience using polars for some of the work that I am doing by comparing my polars implementation of a data analysis pipeline to the equivalent pipeline using pandas. The emphasis here is on the fact that it is my implementation. I am sure that both the polars and the pandas implementation can be improved or are not necessarily following best practices. Moreover, I am barely fluent in polars at this point.\nNevertheless, I think that I learned something for myself and have formed some opinion on things I like and dislike."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#setup",
    "href": "contents/posts/polars-vs-pandas/index.html#setup",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Setup",
    "text": "Setup\nI want to briefly discuss the data that I am encountering for this case study, as well as the steps in the analysis that I am performing.\nThe data I am using here is stored in a parquet file and the resulting data frame as approximately 40k rows with some 100 columns.\nIn a simplified way, that data looks like the following frame. There are two keys which contain measurements (thing of the first key of measurements with recording device A and B, and the second key of different days of the recordings.) Each measurement is a time series with columns t and time representing the local and global time, respectively. At those points in time, signals y1, y2, y3, … are recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkey1\nkey2\nt\ny1\ny2\ny3\ntime\n\n\n\n\n0\nA\nU\n0\n0.342872\n0.731905\n0.341766\n02/02/2023 15:07:21.68\n\n\n1\nA\nV\n1\n0.25941\n0.493496\n0.434559\n02/02/2023 15:07:21.88\n\n\n2\nA\nW\n2\n0.485956\n0.550383\n0.521913\n02/02/2023 15:07:22.28\n\n\n3\nA\nX\n3\n0.210544\n0.406669\n0.540021\n02/02/2023 15:07:22.58\n\n\n4\nB\nU\n2\n0.830654\n0.0386757\n0.635353\n02/02/2023 15:07:22.88\n\n\n5\nB\nV\n3\n0.187675\n0.919848\n0.648574\n02/02/2023 15:07:23.28\n\n\n6\nB\nW\n4\n0.506172\n0.93743\n0.554965\n02/02/2023 15:07:23.58\n\n\n7\nB\nX\n5\n0.21009\n0.829689\n0.857681\n02/02/2023 15:07:23.88\n\n\n\nThe code below obfuscates the real column names because I don’t want to give away sensitive information. However, It is worth outlining the steps that I am doing in the analysis. These steps include: - The conversion of time stamps to datetime formats. - Grouping data over keys and taking the mean value of the initial N samples of some columns. - Subtracting those means from different columns. - Computing rolling means of some columns and using those computed means to replace data in those rows where the mean is below a certain threshold. - Compute a bunch of derived columns that use one or more of the existing columns in some transformation. Those are row wise operations that require no grouping or anything fancy."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#using-polars",
    "href": "contents/posts/polars-vs-pandas/index.html#using-polars",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Using polars",
    "text": "Using polars\nHere’s my implementation using polars for a total of 50 lines of code (LOC). I scan the data instead of reading it directly to run the whole pipeline in a lazy way. Only the call to collect at the end actually forces a computation. Internally, the operations can be optimized and made more efficient. I really like the chaining of operations. While it is somewhat verbose, it is consistent: Every new operation just gets chained to the existing operations with the .keyword() syntax. Computing averages over groups with the .median().over() syntax feels nicer than the pandas equivalent of .groupby().transform().\nCreating new columns with the .with_columns() syntax has the downside, that you need to chain multiple calls to .with_columns() after another if you want to access a column that was created in a prior computation. This is also the reason why my polars implementation has roughly twice the number of LOC when compared with the pandas implementation.\nOne downside that I saw is that, different from pandas, I do not get any kind of auto-complete for the columns that are in a data frame when using the pl.col(\"column name\") syntax. In pandas, VSCode will allow you to auto-complete the column name if you start typing df[\"column and column_name will pop up as a suggestion if it is an element of the data frame.\nlazy_frame = (\n    pl.scan_parquet(path)\n    .rename(mapping={\"column1\": \"criteria2\"})\n    .with_columns(\n        [\n            pl.col(\"time\").str.strptime(pl.Datetime, fmt=\"%m/%d/%Y %H:%M:%S%.f\"),\n            (pl.col(\"y1\") * 16.7).alias(\"z1\"),\n        ]\n    )\n    .with_columns(\n        [\n            pl.col(\"y2\").head(20).median().over(\"criteria1\", \"criteria2\").alias(\"z3\"),\n            pl.col(\"z1\").head(20).median().over(\"criteria1\", \"criteria2\").alias(\"z4\"),\n            pl.col(\"t\").min().over(\"criteria1\", \"criteria2\").alias(\"z5\"),\n        ]\n    )\n    .with_columns((pl.col(\"z4\") - pl.col(\"z3\")).alias(\"z6\"))\n    .with_columns(\n        [\n            (pl.col(\"z1\") - pl.col(\"z6\")).alias(\"y8\"),\n            (pl.col(\"y2\").rolling_min(8, center=True) &lt; 10).alias(\"z10\"),\n            (pl.col(\"t\") - pl.col(\"z5\")).alias(\"t\"),\n            pl.when(pl.col(\"y2\").rolling_min(8, center=True) &lt; 10)\n            .then(float(\"nan\"))\n            .otherwise(pl.col(\"y2\"))\n            .alias((\"z2\")),\n        ]\n    )\n    .with_columns(\n        [\n            (2 * 3.14159 / 60 * pl.col(\"y2\") * pl.col(\"y4\")).alias(\"z8\"),\n            (2 * 3.14159 / 60 * pl.col(\"z2\") * pl.col(\"y4\")).alias((\"z7\")),\n            (2 * 3.14159 / 60 * pl.col(\"y8\") * pl.col(\"y4\")).alias(\"y9\"),\n        ]\n    )\n    .with_columns((pl.col(\"y9\") - pl.col(\"z7\")).alias(\"Error\"))\n    .with_columns(\n        (100 * pl.col(\"Error\") / pl.col(\"z7\")).alias(\"Percentage Error\"),\n    )\n    .with_columns(((0.5 * pl.col(\"y10\") + 0.5 * pl.col(\"y11\"))).alias(\"z13\"))\n    .with_columns((313 * pl.col(\"y12\") / (pl.col(\"z13\") + 273)).alias(\"z12\"))\n    .select([\"a list of some 10 columns that we want to preserve\"])\n)\nframe = lazy_frame.collect()"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#using-pandas",
    "href": "contents/posts/polars-vs-pandas/index.html#using-pandas",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Using pandas",
    "text": "Using pandas\nAnd here’s the pandas implementation for a total of 26 LOC. The data frames are equal up to a small difference in the way rolling is treated between polars and pandas. Overall, this implementation is much more dense. This implementation is not making use of laziness.\ndf = pd.read_parquet(path)\ndf[\"time\"] = pd.to_datetime(df[\"time\"], format=\"%m/%d/%Y %H:%M:%S.%f\")\ndf.rename(columns={\"column1\": \"criteria2\"}, inplace=True)\ndf[\"z1\"] = 16.7 * df[\"y1\"]\ndf[[\"z3\", \"z4\"]] = df.groupby([\"criteria1\", \"criteria2\"])[[\"y2\", \"z1\"]].transform(\n    lambda x: x.head(20).median()\n)[[\"y2\", \"z1\"]]\ndf[\"z5\"] = df.groupby([\"criteria1\", \"criteria2\"])[\"t\"].transform(lambda x: x.min())\ndf[\"z6\"] = df[\"z4\"] - df[\"z3\"]\ndf[\"y8\"] = df[\"z1\"] - df[\"z6\"]\ndf[\"z10\"] = df[\"y2\"].rolling(8, center=True).min() &lt; 10\ndf[\"z2\"] = df[\"y2\"].copy()\ndf.loc[\n    df[\"y2\"].rolling(8, center=True).min() &lt; 10,\n    \"z2\",\n] = float(\"nan\")\ndf[\"t\"] -= df[\"z5\"]\ndf[\"z8\"] = 2 * 3.14159 / 60 * df[\"y2\"] * df[\"y4\"]\ndf[\"z7\"] = 2 * 3.14159 / 60 * df[\"z2\"] * df[\"y4\"]\ndf[\"y9\"] = 2 * 3.14159 / 60 * df[\"y8\"] * df[\"y4\"]\ndf[\"Error\"] = df[\"y9\"] - df[\"z7\"]\ndf[\"Percentage Error\"] = 100 * df[\"Error\"] / df[\"z7\"]\ndf[\"z13\"] = 0.5 * df[\"y10\"] + 0.5 * df[\"y11\"]\ndf[\"z12\"] = 313 * df[\"y12\"] / (df[\"z13\"] + 273)\ndf = df[[\"a list of some 10 columns that we want to preserve\"]]"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#timing-results",
    "href": "contents/posts/polars-vs-pandas/index.html#timing-results",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Timing results",
    "text": "Timing results\nUltimately, a promise of polars is its speed. Running both examples 10 times gives the following timing results:\npolars: 0.021s +- 0.001s\npandas: 0.181s +- 0.006s\nratios: 8.700  +- 0.375\nThat is, the polars implementation runs about 8 to 9 times faster on my 2022 MacBook Pro with an M1 Max chip and 32 GB of RAM. I am using python 3.10, pandas 1.4.3 and polars 0.16.9. Additionally, I created a larger parquet file by just concatenating the original data frame together for one hundred times, i.e.\ndf = pd.read_parquet(\"./parquet/reduced_data.parquet\")\nlargedf = pd.concat([df]*100)\nlargedf.to_parquet(\"./parquet/100reduced_data.parquet\")\nThis is of course not representing an actual dataset that is 100 times larger, but it at least shows a trend of the performance for a potentially larger dataset. Here we’re getting the following timings\npolars: 1.028s +- 0.026s\npandas: 18.813s +- 0.403s\nratios: 18.315  +- 0.490\nThe ratio between both implementations has grown when going from 40k rows to 4.4M rows. (Interestingly, the files sizes of the parquet files are 11MB and 421MB, respectively. Not an increase by 100x.)"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#summary",
    "href": "contents/posts/polars-vs-pandas/index.html#summary",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Summary",
    "text": "Summary\nI want to summarize two things, performance and writing code.\nPerformance wise, my polars implementation is a factor 8 to 9 faster for my example. It seems like this might be a lower bound when moving to larger files but I will keep an eye on that. Given that I am only writing polars code since a month, I am happy with this performance gain.\nFrom an implementation point of view, I was much faster using pandas. I have been using pandas for two years now and I guess that’s just showing here. I really appreciate the auto complete feature for the column names which is a feature that I miss when using polars.\nI do like the chaining of operations, but it makes the code longer and it is a bit annoying having to write code like this (using pandas):\ndf[\"y\"] = 123 + df[\"x\"]\ndf[\"z\"] = 456 + df[\"y\"]\nin this way (using polars):\ndf.with_columns((pl.col(\"x\") + 123).alias(\"y\"))\n    .with_columns((pl.col(\"y\") + 456).alias(\"z\"))\nEspecially with long column names and black formatting, this can double the number of LOC.\nOverall I am quite happy with my polars experience and I will continue using it for project in the futures. One thing I hope for, is that altair can be used with polars data frames instead of needing to call .to_pandas() when passing the data to a chart.\nThanks for reading :)"
  },
  {
    "objectID": "contents/posts/interactive-regression/interactive_regression.html",
    "href": "contents/posts/interactive-regression/interactive_regression.html",
    "title": "Interactive regression",
    "section": "",
    "text": "This example shows how to create an interactive plot with regression lines using altair. The regression lines will be computed over the window that is selected and update accordingly when moving the selected region.\n\n\nThis code block contains all import statements.\nimport polars as pl\nimport numpy as np\nimport altair as alt\nfrom vega_datasets import data\n\n\nLet’s look at a sample data set from vega_datasets, inspired by an example from the altair documentation.\n\nsource = data.iris()\nchart = (\n    alt.Chart(source)\n    .mark_circle()\n    .encode(\n        alt.X(\"sepalLength\").scale(zero=False),\n        alt.Y(\"sepalWidth\").scale(zero=False, padding=1),\n        color=\"species\",\n    )\n    .properties(width=600)\n)\nchart\n\n\n\n\n\n\n\nNow we can add a linear regression line very easily\n\nregression = chart.transform_regression(\n    \"sepalLength\",\n    \"sepalWidth\",\n    groupby=[\"species\"],\n    method=\"poly\",\n    order=5,\n).mark_line()\nalt.layer(chart, regression)\n\n\n\n\n\n\n\nNow let’s allow for some interactivity. We want to be able to select points and have the regression line be updated on that selection of points. However, we still want to plot it over the full domain, that’s why we have to set extent. We’ll make the stroke of the extrapolation range dashed and the interpolation range solid.\n\nbrush = alt.selection_interval()\n\n\nchart = (\n    alt.Chart(source)\n    .mark_circle()\n    .encode(\n        alt.X(\"sepalLength\").scale(domain=(4, 8)),\n        alt.Y(\"sepalWidth\").scale(domain=(1, 6)),\n        color=\"species:N\",\n    )\n    .properties(width=600)\n    .add_params(brush)\n)\n\nregression_solid = (\n    chart.transform_filter(brush)\n    .transform_regression(\n        \"sepalLength\",\n        \"sepalWidth\",\n        groupby=[\"species\"],\n        method=\"poly\",\n        order=3,\n    )\n    .mark_line(clip=True)\n)\n\nregression_dash = (\n    chart.transform_filter(brush)\n    .transform_regression(\n        \"sepalLength\",\n        \"sepalWidth\",\n        groupby=[\"species\"],\n        method=\"poly\",\n        order=3,\n        extent=[4, 8],\n    )\n    .mark_line(clip=True, strokeDash=[5, 5])\n)\n\n\nalt.layer(\n    chart.encode(\n        opacity=alt.condition(brush, alt.value(1.0), alt.value(0.1)),\n    ),\n    regression_dash,\n    regression_solid,\n)"
  },
  {
    "objectID": "contents/datasets.html",
    "href": "contents/datasets.html",
    "title": "Data sets",
    "section": "",
    "text": "World Athletics\nSome 400k+ entries with performances of track and field athletes.\nTour de France\nEvery cyclist and stage of the Tour de France (up to including 2022) in two CSV files.\nAlltime Athletics\nScrapes Peter Larsson’s website Alltime Athletics.\nAbiturnoten data set\nGerman Abitur: Grade distribution over time."
  },
  {
    "objectID": "contents/blog.html",
    "href": "contents/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Nutriscore analysis\n\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nDas Verkehrsproblem in Balve lösen\n\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nInteractive regression\n\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nAbiturnoten inflation in Germany\n\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nTrack and Field World Record Progression\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nComparing polars with pandas; my personal case study\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nLogitech MX Mechanical Mini for Mac: Issues with M1 Mac\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\nNo matching items"
  }
]