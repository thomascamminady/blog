[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "It’s me, hi!",
    "section": "",
    "text": "It’s me, hi!\nMy name is Thomas, I’m a runner, mathematician, and currently working as a data scientist at Wahoo Fitness LLC. I studied Computational Engineering Science at RWTH Aachen University and then went ont to get a Ph.D. in applied mathematics from the Karlsruhe Institute of Technology.\n\n\n\nWhen I’m not using pandas (or polars!) to turn wide dataframes into long ones or spend too much time thinking about colors, I like to run through forests and around tracks.\nReach out via email if you want to contact me!"
  },
  {
    "objectID": "contents/datasets.html",
    "href": "contents/datasets.html",
    "title": "Projects",
    "section": "",
    "text": "Some 400k+ entries with performances of track and field athletes.\n\n\n\nEvery cyclist and stage of the Tour de France (up to including 2022) in two CSV files.\n\n\n\nScrapes Peter Larsson’s website Alltime Athletics.\n\n\n\nGerman Abitur: Grade distribution over time."
  },
  {
    "objectID": "contents/datasets.html#data-sets",
    "href": "contents/datasets.html#data-sets",
    "title": "Projects",
    "section": "",
    "text": "Some 400k+ entries with performances of track and field athletes.\n\n\n\nEvery cyclist and stage of the Tour de France (up to including 2022) in two CSV files.\n\n\n\nScrapes Peter Larsson’s website Alltime Athletics.\n\n\n\nGerman Abitur: Grade distribution over time."
  },
  {
    "objectID": "contents/datasets.html#code",
    "href": "contents/datasets.html#code",
    "title": "Projects",
    "section": "Code",
    "text": "Code\n\ndicopal\nA collection of colors, an homage to the dicopal.js project. This website allows you to quickly browse a variety of color palettes and just copy their hex codes.\n\n\ncoolors preview\nPreview color palettes from coolors.co.\n\n\nKaleidoscopic\nThis will become a webapp to display color palettes."
  },
  {
    "objectID": "contents/posts/heatmaps/main.html",
    "href": "contents/posts/heatmaps/main.html",
    "title": "Quickly creating heatmaps with pandas",
    "section": "",
    "text": "panda’s style functionality is often a good enough replacement for a more sophisticated plotting library when all we want is a simple heatmap of two-dimensional data.\nLet’s say we have this fake time series data, were for each year and month, multiple measurements are recorded.\n\nimport pandas as pd\nimport numpy as np\n\nn = 1_000\ndf = pd.DataFrame(\n    {\n        \"year\": np.random.randint(2000, 2020, n),\n        \"month\": np.random.randint(1, 13, n),\n        \"measurement\": np.random.randn(n),\n    }\n)\n\nWe want to create a heatmap, showing the averages of each year-month-combination.\nLet’s aggregate measurements from the same months and years and then create a pivot table.\n\ndf_pivot = (\n    df.groupby([\"year\", \"month\"])\n    .sum()\n    .reset_index()\n    .pivot(index=\"year\", columns=\"month\", values=\"measurement\")\n    .reset_index()\n)\n\nNow, we can use .style.background_gradient to color the output. Additionally, let’s use .format to only show one digit after the decimal point.\n\ndf_pivot.style.format(\n    precision=1,\n).background_gradient(\n    cmap=\"RdBu\",\n    vmin=-1,\n    vmax=1,\n    subset=[i for i in range(1, 13)],\n)\n\n\n\n\n\n\nmonth\nyear\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n0\n2000\n3.5\n-1.5\n0.9\n2.1\n1.6\n-1.0\n-2.3\n-4.3\n-1.6\n-3.6\n-0.4\n1.3\n\n\n1\n2001\n1.1\n1.2\n0.3\n-0.8\n1.1\n2.0\n0.2\n-1.2\n2.8\n0.8\n2.8\n1.6\n\n\n2\n2002\n-1.0\n0.7\n-1.2\n-3.0\n-3.3\n0.0\n-1.1\n0.8\n2.2\n-3.1\n-3.6\n-3.1\n\n\n3\n2003\n2.5\n0.7\n-2.8\n-0.3\n0.4\n1.8\n-0.2\n0.0\n-0.0\n-2.1\n1.7\n-0.4\n\n\n4\n2004\n-4.3\n1.0\n-0.5\n-2.7\n2.2\n0.7\n0.7\n4.8\n0.6\n2.1\n-0.5\n1.2\n\n\n5\n2005\n-1.3\n2.5\n1.1\n1.9\n1.6\n-1.1\n-1.9\n-1.0\n0.1\n-1.3\n-0.7\n0.3\n\n\n6\n2006\n2.4\n-1.1\n-0.7\n4.5\n-0.2\n-0.7\n0.9\n-1.6\n-0.9\n-1.2\n1.6\n0.3\n\n\n7\n2007\n-5.1\n-2.2\n0.7\n-6.0\n-0.0\n3.9\n-0.4\n0.5\n2.0\n3.1\n1.1\n-2.6\n\n\n8\n2008\n1.2\n-2.8\n4.9\n0.7\n-0.5\n3.4\n-0.6\n2.1\n0.6\n-0.6\n1.3\n-0.2\n\n\n9\n2009\n-2.0\n0.8\n2.3\n0.7\nnan\n0.6\n-1.1\n-0.6\n1.3\n2.1\n-0.8\n0.7\n\n\n10\n2010\n-1.9\n-1.4\n-0.1\n-1.6\n0.3\n1.4\n1.0\n0.7\n1.2\n0.5\n-0.3\n-0.6\n\n\n11\n2011\n0.3\n1.1\n0.6\n-2.6\n0.1\n-3.2\n3.9\n-1.5\n0.8\n2.0\n-1.9\n-4.6\n\n\n12\n2012\n-0.6\n-1.2\n0.3\n2.0\n-0.7\n1.0\n-1.1\n2.2\n6.4\n-3.9\n-0.3\n0.3\n\n\n13\n2013\n2.7\n3.4\n0.3\n0.2\n1.1\n-3.6\n0.8\n1.3\n-0.8\n-1.3\n-3.3\n-1.0\n\n\n14\n2014\n0.1\n-0.2\n-0.9\n-2.3\n-3.5\n-1.8\n-4.4\n-0.6\n1.4\n-1.3\n1.6\n-3.2\n\n\n15\n2015\n-1.5\n0.6\n1.7\n-0.5\n-2.4\n0.7\n1.0\n1.8\n0.1\n-0.3\n-1.6\n1.1\n\n\n16\n2016\n-2.5\n-0.9\n-4.6\n0.1\n2.5\n-0.6\n-0.8\n-1.0\n-0.2\n-2.3\n0.2\n0.2\n\n\n17\n2017\n-1.1\n4.0\n-5.6\n-2.3\n0.4\n-1.6\n1.0\n-0.1\n-0.6\n0.7\n-0.9\nnan\n\n\n18\n2018\n-0.5\n1.0\n-3.1\n-0.7\n-1.9\n-0.9\n-3.2\n2.3\n1.3\n2.1\n2.6\nnan\n\n\n19\n2019\n1.6\n-0.2\n2.2\n1.2\n1.9\n0.8\n-1.2\n0.2\n0.5\n5.7\n-2.1\n-3.5\n\n\n\n\n\nThat’s already good enough in a lot of cases :)"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html",
    "href": "contents/posts/polars-vs-pandas/index.html",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "",
    "text": "A couple of weeks ago I came across polars, a “Lightning-fast DataFrame library for Rust and Python.” Since then, I have been playing around with it, trying to do some of my daily data analyses tasks with polars instead of pandas.\nI wanted to summarize my experience using polars for some of the work that I am doing by comparing my polars implementation of a data analysis pipeline to the equivalent pipeline using pandas. The emphasis here is on the fact that it is my implementation. I am sure that both the polars and the pandas implementation can be improved or are not necessarily following best practices. Moreover, I am barely fluent in polars at this point.\nNevertheless, I think that I learned something for myself and have formed some opinion on things I like and dislike."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#preface",
    "href": "contents/posts/polars-vs-pandas/index.html#preface",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "",
    "text": "A couple of weeks ago I came across polars, a “Lightning-fast DataFrame library for Rust and Python.” Since then, I have been playing around with it, trying to do some of my daily data analyses tasks with polars instead of pandas.\nI wanted to summarize my experience using polars for some of the work that I am doing by comparing my polars implementation of a data analysis pipeline to the equivalent pipeline using pandas. The emphasis here is on the fact that it is my implementation. I am sure that both the polars and the pandas implementation can be improved or are not necessarily following best practices. Moreover, I am barely fluent in polars at this point.\nNevertheless, I think that I learned something for myself and have formed some opinion on things I like and dislike."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#setup",
    "href": "contents/posts/polars-vs-pandas/index.html#setup",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Setup",
    "text": "Setup\nI want to briefly discuss the data that I am encountering for this case study, as well as the steps in the analysis that I am performing.\nThe data I am using here is stored in a parquet file and the resulting data frame as approximately 40k rows with some 100 columns.\nIn a simplified way, that data looks like the following frame. There are two keys which contain measurements (thing of the first key of measurements with recording device A and B, and the second key of different days of the recordings.) Each measurement is a time series with columns t and time representing the local and global time, respectively. At those points in time, signals y1, y2, y3, … are recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkey1\nkey2\nt\ny1\ny2\ny3\ntime\n\n\n\n\n0\nA\nU\n0\n0.342872\n0.731905\n0.341766\n02/02/2023 15:07:21.68\n\n\n1\nA\nV\n1\n0.25941\n0.493496\n0.434559\n02/02/2023 15:07:21.88\n\n\n2\nA\nW\n2\n0.485956\n0.550383\n0.521913\n02/02/2023 15:07:22.28\n\n\n3\nA\nX\n3\n0.210544\n0.406669\n0.540021\n02/02/2023 15:07:22.58\n\n\n4\nB\nU\n2\n0.830654\n0.0386757\n0.635353\n02/02/2023 15:07:22.88\n\n\n5\nB\nV\n3\n0.187675\n0.919848\n0.648574\n02/02/2023 15:07:23.28\n\n\n6\nB\nW\n4\n0.506172\n0.93743\n0.554965\n02/02/2023 15:07:23.58\n\n\n7\nB\nX\n5\n0.21009\n0.829689\n0.857681\n02/02/2023 15:07:23.88\n\n\n\nThe code below obfuscates the real column names because I don’t want to give away sensitive information. However, It is worth outlining the steps that I am doing in the analysis. These steps include: - The conversion of time stamps to datetime formats. - Grouping data over keys and taking the mean value of the initial N samples of some columns. - Subtracting those means from different columns. - Computing rolling means of some columns and using those computed means to replace data in those rows where the mean is below a certain threshold. - Compute a bunch of derived columns that use one or more of the existing columns in some transformation. Those are row wise operations that require no grouping or anything fancy."
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#using-polars",
    "href": "contents/posts/polars-vs-pandas/index.html#using-polars",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Using polars",
    "text": "Using polars\nHere’s my implementation using polars for a total of 50 lines of code (LOC). I scan the data instead of reading it directly to run the whole pipeline in a lazy way. Only the call to collect at the end actually forces a computation. Internally, the operations can be optimized and made more efficient. I really like the chaining of operations. While it is somewhat verbose, it is consistent: Every new operation just gets chained to the existing operations with the .keyword() syntax. Computing averages over groups with the .median().over() syntax feels nicer than the pandas equivalent of .groupby().transform().\nCreating new columns with the .with_columns() syntax has the downside, that you need to chain multiple calls to .with_columns() after another if you want to access a column that was created in a prior computation. This is also the reason why my polars implementation has roughly twice the number of LOC when compared with the pandas implementation.\nOne downside that I saw is that, different from pandas, I do not get any kind of auto-complete for the columns that are in a data frame when using the pl.col(\"column name\") syntax. In pandas, VSCode will allow you to auto-complete the column name if you start typing df[\"column and column_name will pop up as a suggestion if it is an element of the data frame.\nlazy_frame = (\n    pl.scan_parquet(path)\n    .rename(mapping={\"column1\": \"criteria2\"})\n    .with_columns(\n        [\n            pl.col(\"time\").str.strptime(pl.Datetime, fmt=\"%m/%d/%Y %H:%M:%S%.f\"),\n            (pl.col(\"y1\") * 16.7).alias(\"z1\"),\n        ]\n    )\n    .with_columns(\n        [\n            pl.col(\"y2\").head(20).median().over(\"criteria1\", \"criteria2\").alias(\"z3\"),\n            pl.col(\"z1\").head(20).median().over(\"criteria1\", \"criteria2\").alias(\"z4\"),\n            pl.col(\"t\").min().over(\"criteria1\", \"criteria2\").alias(\"z5\"),\n        ]\n    )\n    .with_columns((pl.col(\"z4\") - pl.col(\"z3\")).alias(\"z6\"))\n    .with_columns(\n        [\n            (pl.col(\"z1\") - pl.col(\"z6\")).alias(\"y8\"),\n            (pl.col(\"y2\").rolling_min(8, center=True) &lt; 10).alias(\"z10\"),\n            (pl.col(\"t\") - pl.col(\"z5\")).alias(\"t\"),\n            pl.when(pl.col(\"y2\").rolling_min(8, center=True) &lt; 10)\n            .then(float(\"nan\"))\n            .otherwise(pl.col(\"y2\"))\n            .alias((\"z2\")),\n        ]\n    )\n    .with_columns(\n        [\n            (2 * 3.14159 / 60 * pl.col(\"y2\") * pl.col(\"y4\")).alias(\"z8\"),\n            (2 * 3.14159 / 60 * pl.col(\"z2\") * pl.col(\"y4\")).alias((\"z7\")),\n            (2 * 3.14159 / 60 * pl.col(\"y8\") * pl.col(\"y4\")).alias(\"y9\"),\n        ]\n    )\n    .with_columns((pl.col(\"y9\") - pl.col(\"z7\")).alias(\"Error\"))\n    .with_columns(\n        (100 * pl.col(\"Error\") / pl.col(\"z7\")).alias(\"Percentage Error\"),\n    )\n    .with_columns(((0.5 * pl.col(\"y10\") + 0.5 * pl.col(\"y11\"))).alias(\"z13\"))\n    .with_columns((313 * pl.col(\"y12\") / (pl.col(\"z13\") + 273)).alias(\"z12\"))\n    .select([\"a list of some 10 columns that we want to preserve\"])\n)\nframe = lazy_frame.collect()"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#using-pandas",
    "href": "contents/posts/polars-vs-pandas/index.html#using-pandas",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Using pandas",
    "text": "Using pandas\nAnd here’s the pandas implementation for a total of 26 LOC. The data frames are equal up to a small difference in the way rolling is treated between polars and pandas. Overall, this implementation is much more dense. This implementation is not making use of laziness.\ndf = pd.read_parquet(path)\ndf[\"time\"] = pd.to_datetime(df[\"time\"], format=\"%m/%d/%Y %H:%M:%S.%f\")\ndf.rename(columns={\"column1\": \"criteria2\"}, inplace=True)\ndf[\"z1\"] = 16.7 * df[\"y1\"]\ndf[[\"z3\", \"z4\"]] = df.groupby([\"criteria1\", \"criteria2\"])[[\"y2\", \"z1\"]].transform(\n    lambda x: x.head(20).median()\n)[[\"y2\", \"z1\"]]\ndf[\"z5\"] = df.groupby([\"criteria1\", \"criteria2\"])[\"t\"].transform(lambda x: x.min())\ndf[\"z6\"] = df[\"z4\"] - df[\"z3\"]\ndf[\"y8\"] = df[\"z1\"] - df[\"z6\"]\ndf[\"z10\"] = df[\"y2\"].rolling(8, center=True).min() &lt; 10\ndf[\"z2\"] = df[\"y2\"].copy()\ndf.loc[\n    df[\"y2\"].rolling(8, center=True).min() &lt; 10,\n    \"z2\",\n] = float(\"nan\")\ndf[\"t\"] -= df[\"z5\"]\ndf[\"z8\"] = 2 * 3.14159 / 60 * df[\"y2\"] * df[\"y4\"]\ndf[\"z7\"] = 2 * 3.14159 / 60 * df[\"z2\"] * df[\"y4\"]\ndf[\"y9\"] = 2 * 3.14159 / 60 * df[\"y8\"] * df[\"y4\"]\ndf[\"Error\"] = df[\"y9\"] - df[\"z7\"]\ndf[\"Percentage Error\"] = 100 * df[\"Error\"] / df[\"z7\"]\ndf[\"z13\"] = 0.5 * df[\"y10\"] + 0.5 * df[\"y11\"]\ndf[\"z12\"] = 313 * df[\"y12\"] / (df[\"z13\"] + 273)\ndf = df[[\"a list of some 10 columns that we want to preserve\"]]"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#timing-results",
    "href": "contents/posts/polars-vs-pandas/index.html#timing-results",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Timing results",
    "text": "Timing results\nUltimately, a promise of polars is its speed. Running both examples 10 times gives the following timing results:\npolars: 0.021s +- 0.001s\npandas: 0.181s +- 0.006s\nratios: 8.700  +- 0.375\nThat is, the polars implementation runs about 8 to 9 times faster on my 2022 MacBook Pro with an M1 Max chip and 32 GB of RAM. I am using python 3.10, pandas 1.4.3 and polars 0.16.9. Additionally, I created a larger parquet file by just concatenating the original data frame together for one hundred times, i.e.\ndf = pd.read_parquet(\"./parquet/reduced_data.parquet\")\nlargedf = pd.concat([df]*100)\nlargedf.to_parquet(\"./parquet/100reduced_data.parquet\")\nThis is of course not representing an actual dataset that is 100 times larger, but it at least shows a trend of the performance for a potentially larger dataset. Here we’re getting the following timings\npolars: 1.028s +- 0.026s\npandas: 18.813s +- 0.403s\nratios: 18.315  +- 0.490\nThe ratio between both implementations has grown when going from 40k rows to 4.4M rows. (Interestingly, the files sizes of the parquet files are 11MB and 421MB, respectively. Not an increase by 100x.)"
  },
  {
    "objectID": "contents/posts/polars-vs-pandas/index.html#summary",
    "href": "contents/posts/polars-vs-pandas/index.html#summary",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Summary",
    "text": "Summary\nI want to summarize two things, performance and writing code.\nPerformance wise, my polars implementation is a factor 8 to 9 faster for my example. It seems like this might be a lower bound when moving to larger files but I will keep an eye on that. Given that I am only writing polars code since a month, I am happy with this performance gain.\nFrom an implementation point of view, I was much faster using pandas. I have been using pandas for two years now and I guess that’s just showing here. I really appreciate the auto complete feature for the column names which is a feature that I miss when using polars.\nI do like the chaining of operations, but it makes the code longer and it is a bit annoying having to write code like this (using pandas):\ndf[\"y\"] = 123 + df[\"x\"]\ndf[\"z\"] = 456 + df[\"y\"]\nin this way (using polars):\ndf.with_columns((pl.col(\"x\") + 123).alias(\"y\"))\n    .with_columns((pl.col(\"y\") + 456).alias(\"z\"))\nEspecially with long column names and black formatting, this can double the number of LOC.\nOverall I am quite happy with my polars experience and I will continue using it for project in the futures. One thing I hope for, is that altair can be used with polars data frames instead of needing to call .to_pandas() when passing the data to a chart.\nThanks for reading :)"
  },
  {
    "objectID": "contents/posts/world-records/records.html",
    "href": "contents/posts/world-records/records.html",
    "title": "Track and Field World Record Progression",
    "section": "",
    "text": "The figure below show how the track and field world records have progressed over time.\nClick on the legend to highlight individual events and see the respective world record holders.\n\n\nCode\nimport altair as alt\nimport polars as pl\nfrom alltime_athletics_python.io import download_data\nfrom alltime_athletics_python.io import import_running_only_events\n\n# download_data()\ndf = import_running_only_events(\"./data\")\n\n\n\n\nCode\nworld_records = (\n    df.filter(pl.col(\"event\").str.contains(\"walk\") == False)\n    .filter(pl.col(\"event type\") == \"standard\")\n    .sort(\"sex\", \"distance\", \"event\", \"date of event\")\n    .with_columns(\n        pl.col(\"result seconds\")\n        .cummin()\n        .over(\"sex\", \"event\")\n        .alias(\"world record time\")\n    )\n    .filter(pl.col(\"result seconds\") == pl.col(\"world record time\"))\n    .groupby(\"sex\", \"event\", \"result seconds\", maintain_order=True)\n    .first()\n    .with_columns(\n        (\n            100\n            * pl.col(\"result seconds\")\n            / pl.col(\"result seconds\").min().over(\"sex\", \"event\")\n        ).alias(\"percent of wr\")\n    )\n)\n\nworld_records = pl.concat(\n    [\n        world_records,\n        world_records.filter(pl.col(\"rank\") == 1).with_columns(\n            [\n                pl.lit(\"2023-06-07\")\n                .str.strptime(pl.Date, format=\"%Y-%m-%d\")\n                .alias(\"date of event\"),\n                pl.lit(-1).cast(pl.Int64).alias(\"rank\"),\n            ]\n        ),\n    ]\n).with_columns(pl.col(\"sex\").apply(lambda s: s.title()))\n\ndata = world_records.select(\n    \"date of event\", \"percent of wr\", \"event\", \"sex\", \"name\", \"rank\"\n).to_pandas()\nlegend_selection = alt.selection_point(fields=[\"event\"], bind=\"legend\")\nlegend_selection_empty = alt.selection_point(\n    fields=[\"event\"], bind=\"legend\", empty=False\n)\n\nbase = (\n    alt.Chart(data)\n    .encode(\n        x=alt.X(\"date of event:T\")\n        .scale(domain=(\"1950-01-01\", \"2026-01-01\"))\n        .title(\"Year\"),\n        y=alt.Y(\"percent of wr:Q\")\n        .scale(domain=(100, 110))\n        .axis(values=list(range(100, 120, 2)))\n        .title(\"Time in % of current WR\"),\n        color=alt.Color(\n            \"event:N\",\n            sort=world_records.sort(\"distance\")[\"event\"]\n            .unique(maintain_order=True)\n            .to_list(),\n        )\n        .scale(scheme=\"dark2\")\n        .title(\"Event\"),\n        # strokeDash=\"sex:N\",\n        opacity=alt.condition(legend_selection, alt.value(1), alt.value(0)),\n    )\n    .properties(width=800, height=500)\n    .add_params(legend_selection)\n    .add_params(legend_selection_empty)\n)\n\nbase_no_endpoint = base.transform_filter(alt.datum[\"rank\"] &gt; 0)\n\ntext = base_no_endpoint.encode(\n    text=\"name:N\",\n    opacity=alt.condition(legend_selection_empty, alt.value(0.9), alt.value(0.0)),\n)\n\n\nchart = (\n    alt.layer(\n        base.mark_line(interpolate=\"step-after\", clip=True, strokeWidth=3),\n        base_no_endpoint.mark_point(filled=True, clip=True, size=100),\n        text.mark_text(clip=True, fontSize=14, angle=270 + 45, align=\"left\", dx=15),\n    )\n    .facet(\n        row=alt.Row(\"sex:N\").title(\"\").header(labelAngle=0),\n        title=\"World Record Progression\",\n    )\n    .resolve_scale(x=\"independent\")\n)\n\ndisplay(chart)\n\n\n/var/folders/1c/6_s1_dhd2xngnxyrz3vnpqfr0000gq/T/ipykernel_29309/2903824052.py:35: PolarsInefficientApplyWarning:\n\n\nExpr.apply is significantly slower than the native expressions API.\nOnly use if you absolutely CANNOT implement your logic otherwise.\nIn this case, you can replace your `apply` with the following:\n  - pl.col(\"sex\").apply(lambda s: ...)\n  + pl.col(\"sex\").str.to_titlecase()"
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html",
    "href": "contents/posts/logitech-issue/index.html",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "",
    "text": "About two month ago, I bought a Logitech MX Mechanical Mini for Mac and it worked like a charm with my previous Intel Macbook Pro (2019). Last week I received a new work Macbook that uses the newer M1 chip (M1 Max). Since then I am experiencing issues with the connectivity of the MX Mechanical Mini.\nAs of now (Feb 22, 2023) I have no solution for this issue."
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html#whats-the-issue",
    "href": "contents/posts/logitech-issue/index.html#whats-the-issue",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "What’s the issue?",
    "text": "What’s the issue?\nIt’s a connectivity issue, where it seems like the keyboard goes to sleep after only a couple of seconds of inactivity (smaller than ten seconds). Waking up the keyboard then takes some three to four seconds, time during which no keystrokes are recorded. This is especially annoying for me, since I work as a developer and there are frequent periods of not typing (but thinking), followed by quick bursts of typing.\nI did not notice this issue on the Intel-based Macbook which makes me wonder whether this is an issue on the hardware side of things."
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html#what-have-i-tried",
    "href": "contents/posts/logitech-issue/index.html#what-have-i-tried",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "What have I tried?",
    "text": "What have I tried?\nI tried the usual things: - Making sure that the battery is charged. - Updating driver (Logi Options+) - Disconnect the device, forget the device in bluetooth settings, and reconnect. - Factory reset the keyboard, including the removal of all previously paired devices. - Pairing my Mac on each of the 3 different Easy-Switch slots to see whether the slot makes a difference."
  },
  {
    "objectID": "contents/posts/logitech-issue/index.html#references-ressources",
    "href": "contents/posts/logitech-issue/index.html#references-ressources",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "References / Ressources",
    "text": "References / Ressources\nI looked for this issue and found some recent posts / threads on reddit: - MX mechanical sleeps - MX keys randomly disconnecting\nThis website explains how to factory reset your keyboard such that all previously paired devices are forgotten: - Factory reset Logitech MX Keys Keyboard (Note: The sequence is esc O esc O esc B, where O is the fifteenth letter of the alphabet, not the number 0.)"
  },
  {
    "objectID": "contents/posts/gradeinflation/Abiturnoten.html",
    "href": "contents/posts/gradeinflation/Abiturnoten.html",
    "title": "Abiturnoten inflation in Germany",
    "section": "",
    "text": "Code\nfrom create_plot import run\n\nrun()\n\n\nThe German Abitur - colloquially known as the ‘Abitur’ - represents the culmination of a student’s secondary education. This advanced level of high school education concludes with an exam of significant importance. Successfully passing the Abitur opens the door to tertiary education institutions - universities, colleges, and certain professional schools. Given the weightage of this examination, an upward trend in the grades observed over the years deserves a closer inspection.\nRecently, an intriguing analysis was conducted, plotting the trajectory of Abitur grades over time. The resulting graph reveals a surprising trend: while there has been an increase in the number of good grades, the number of mediocre grades has largely remained unchanged, and bad grades have seen a decline. This pattern merits a deeper dive into the phenomenon of grade inflation, a subject that has been a topic of substantial debate in educational circles worldwide.\nGrade inflation refers to the trend where higher grades are awarded for the same quality of work compared to previous years. In essence, the standard of what constitutes a ‘good’ or ‘excellent’ grade is slowly diminished. This can lead to a devaluation of educational attainment, making it more challenging for universities and employers to discern the calibre of graduates. It also raises concerns about the efficacy of the education system itself: are students truly performing better, or are the standards being lowered?\nThe figure produced from the recent analysis offers visual evidence of grade inflation in the Abitur. The rising trend of good grades suggests that more and more students are performing at a level previously considered exceptional. On the surface, this could be interpreted as a positive development, signaling increased student achievement. However, the stagnation of mediocre grades and the decrease in poor grades provide a different perspective.\nWhy are mediocre grades stagnating, and why are bad grades dwindling? If the education system is improving uniformly, we would expect an overall shift with improvements across the board. The specificity of this trend suggests that the bar for what constitutes good performance might be lowering, thereby allowing more students to achieve ‘good’ grades.\nFurthermore, this pattern is potentially problematic for universities and employers who rely on these grades to gauge a student’s competence. If more students are receiving good grades, the differentiation between an average and an outstanding student becomes blurred, reducing the efficacy of grades as a tool for assessing student abilities.\nThis analysis is crucial in sparking further discussion on the phenomenon of grade inflation. It opens up a much-needed discourse on the current educational standards and how they are evolving. It also brings to light the importance of maintaining rigorous, consistent grading standards to ensure that grades remain a reliable measure of student achievement.\nThe journey to understand the German Abitur’s evolving landscape continues, and such investigations provide essential insights. They remind us that while grades are critical, they are just one aspect of education. The ultimate goal should always be the cultivation of knowledge, skills, and character, ensuring that students are truly prepared for the challenges they will face in their futures.\n\n\n\nGrade inflation German Abitur, share of 1.0.\n\n\n\n\n\nGrade inflation German Abitur"
  },
  {
    "objectID": "contents/posts/verkehr/verkehr.html",
    "href": "contents/posts/verkehr/verkehr.html",
    "title": "Das Verkehrsproblem in Balve lösen",
    "section": "",
    "text": "Einfache und günstige Maßnahmen können die Lebensqualität der Bürger:innen drastisch verbessern.\n\nDas Problem\nNicht nur seit der Schließung der Autobahnbrücke gibt es folgende Probleme:\n\nEin enorm starkes Verkehrsaufkommen. Das Kreuzen der Hauptstraße im Stadtzentrum ist de facto nicht sicher möglich.\nZu viele LKW quetschen sich durch die Balver Hauptstraße. Neben dem Sicherheitsaspekt kommt hier auch ein erhöhtes Lärmaufkommen und Unfallgefahr hinzu. Erholung im Stadtgebiet ist nicht möglich.\nKeine Radwege oder Fahrradschutzstreifen entlang der Hauptstraße. Stadtradeln ist zwar eine schöne Idee, sicher fühlt man sich aber nicht.\nEs wird zu häufig zu schnell gefahren. Insbesondere mit parkenden Autos ist Tempo 50 zu hoch.\n\n\n\nEine Lösung\nDie Probleme lassen sich einfach entschärfen. Fahren im Stadtgebiet Balve kann sicherer gemacht werden, indem Tempo 30 eingeführt wird. Zudem verhindert ein LKW-Verbot zwischen Langenholthausen und Balve, dass sich schweres Gefährt durch die Stadtmitte quetscht. Der Verkehr kann problemlos umgeleitet werden. Dies ist in der folgenden Grafik illustriert. Fahrradschutzstreifen machen es für Autofahrer:innen deutlicher, dass nur bei ausreichendem Abstand überholt werden darf. Zudem ist ein Zebrastreifen auf Höhe der Volksbank notwendig um das Überqueren der Hauptstraße wieder sicher zu machen.\n\n\n\nEine mögliche Umleitung\n\n\n\n\nDie Ausreden\nHier soll nur kurz auf mögliche Argumente eingegangen werden.\n\nNicht genügend Platz für Autos.\nFahrräder müssten laut StVO auch jetzt schon innerorts mit 1.5m Abstand überholt werden. Ein Schutzstreifen für Radfahrende würde also nichts an der Platzverteilung im Straßenverkehr ändern.\n\n\nDas ist zu teuer!\nDie Kosten würden sich auf ein paar wenige Schilder und Farbe belaufen.\n\n\nDas liegt in der Hand von Straßen.NRW und wir können da nichts machen.\nSich für die Belange der Bürger:innen einzusetzen ist genau die Aufgabe der lokalen Politiker:innen. Sicherlich lässt sich hier ein Weg finden; andere Städte im näheren Umkreis haben es auch geschafft."
  },
  {
    "objectID": "contents/posts/grammar-of-graphics/main.html",
    "href": "contents/posts/grammar-of-graphics/main.html",
    "title": "Grammar of Graphics in Mathematics",
    "section": "",
    "text": "Say you’re a mathematician and you want to plot \\(sin(x)\\) and \\(sin(x)\\), how would you do that? You’d probably do something like this1:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots()\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), label=\"sin(x)\")\nax.plot(x, np.cos(x), label=\"cos(x)\")\nax.legend()\nax.set(title=\"Trigonometric functions\")\nplt.show()\n\n\n\n\n\n\n\n\nWhat I want to show you in this post is that it’s worth do alter that approach slightly. Let me show you.\n\nimport altair as alt\nimport pandas as pd\n\ndf = pd.concat(\n    [\n        pd.DataFrame({\"x\": x, \"y\": np.sin(x), \"Function\": \"sin\"}),\n        pd.DataFrame({\"x\": x, \"y\": np.cos(x), \"Function\": \"cos\"}),\n    ]\n)\nalt.Chart(df, title=\"Trigonometric functions\").mark_line().encode(\n    x=\"x\",\n    y=\"y\",\n    color=\"Function\",\n)\n\n\n\n\n\n\n\nLet’s unwrap what happens here. As a first note, we are using altair as our plotting library, but something like seaborn,ggplot2, or Plot would have been fine as well.\nNext, we are creating a pandas.DataFrame which is nothing but a table:\n\n\nCode\ndf\n\n\n\n\n\n\n\n\n\nx\ny\nFunction\n\n\n\n\n0\n0.00000\n0.000000\nsin\n\n\n1\n0.10101\n0.100838\nsin\n\n\n2\n0.20202\n0.200649\nsin\n\n\n3\n0.30303\n0.298414\nsin\n\n\n4\n0.40404\n0.393137\nsin\n\n\n...\n...\n...\n...\n\n\n95\n9.59596\n-0.985384\ncos\n\n\n96\n9.69697\n-0.963184\ncos\n\n\n97\n9.79798\n-0.931165\ncos\n\n\n98\n9.89899\n-0.889653\ncos\n\n\n99\n10.00000\n-0.839072\ncos\n\n\n\n\n200 rows × 3 columns\n\n\n\nThe one special thing about this dataframe however, is also the reason for this post. This is a long dataframe, not a wide dataframe. A wide dataframe would look like this:\n\n\nCode\n(\n    df.pivot(index=\"x\", columns=\"Function\", values=\"y\")\n    .reset_index()\n    .rename_axis(None, axis=1)\n)\n\n\n\n\n\n\n\n\n\nx\ncos\nsin\n\n\n\n\n0\n0.00000\n1.000000\n0.000000\n\n\n1\n0.10101\n0.994903\n0.100838\n\n\n2\n0.20202\n0.979663\n0.200649\n\n\n3\n0.30303\n0.954437\n0.298414\n\n\n4\n0.40404\n0.919480\n0.393137\n\n\n...\n...\n...\n...\n\n\n95\n9.59596\n-0.985384\n-0.170347\n\n\n96\n9.69697\n-0.963184\n-0.268843\n\n\n97\n9.79798\n-0.931165\n-0.364599\n\n\n98\n9.89899\n-0.889653\n-0.456637\n\n\n99\n10.00000\n-0.839072\n-0.544021\n\n\n\n\n100 rows × 3 columns\n\n\n\nThis dataframe is wide, because instead of stacking the values for \\(sin\\) and \\(cos\\) on top of another, they are side by side.\nThere are a couple of reasons why the long way is better than the wide way.\n\nA lot of modern visualization tools make heavy use of the Grammar of Graphics2 , an approach that is based on the long format.\nYou can store time series of different lengths in the same dataframe.\nA lot of data transformations (e.g. groupby) are much easier to use this way.\n\nSomething I very much thought of as a potential downside, however, is the different storage that is needed. The wide dataframe needs to store \\(3\\cdot N\\) double values (\\(x\\),\\(sin\\), \\(cos\\)), whereas the long format requires storage for \\(4\\cdot N\\) double values and \\(2\\cdot N\\) string values.\nAnother drawback is the added complexity when thinking about how the data should be stored.\nSo before I try to justify why this extra memory usage and complexity might be justified, let’s extend our example a little to make it slightly more complex. Let’s say we want to compare different frequencies for \\(sin\\) and \\(cos\\).\nWith our initial approach, this could look like this\n\nfig, ax = plt.subplots()\nx = np.linspace(0, 10, 100)\nfor i in range(1, 4):\n    ax.plot(x, np.sin(i * x), label=f\"sin({i}*x)\")\n    ax.plot(x, np.cos(i * x), label=f\"cos({i}*x)\")\nax.legend()\nax.set(title=\"Trigonometric functions\")\nplt.show()\n\n\n\n\n\n\n\n\nNot pretty, but you get the idea.\nHere’s the approach using altair and a long dataframe. First let’s bring the data in the correct form.\n\ndf = pd.concat(\n    [\n        pd.DataFrame(\n            {\n                \"x\": x,\n                \"y\": np.sin(i * x),\n                \"Function\": \"sin\",\n                \"Frequency\": i,\n            }\n        )\n        for i in range(1, 4)\n    ]\n    + [\n        pd.DataFrame(\n            {\n                \"x\": x,\n                \"y\": np.cos(i * x),\n                \"Function\": \"cos\",\n                \"Frequency\": i,\n            }\n        )\n        for i in range(1, 4)\n    ]\n)\ndf\n\n\n\n\n\n\n\n\nx\ny\nFunction\nFrequency\n\n\n\n\n0\n0.00000\n0.000000\nsin\n1\n\n\n1\n0.10101\n0.100838\nsin\n1\n\n\n2\n0.20202\n0.200649\nsin\n1\n\n\n3\n0.30303\n0.298414\nsin\n1\n\n\n4\n0.40404\n0.393137\nsin\n1\n\n\n...\n...\n...\n...\n...\n\n\n95\n9.59596\n-0.871008\ncos\n3\n\n\n96\n9.69697\n-0.684721\ncos\n3\n\n\n97\n9.79798\n-0.436037\ncos\n3\n\n\n98\n9.89899\n-0.147619\ncos\n3\n\n\n99\n10.00000\n0.154251\ncos\n3\n\n\n\n\n600 rows × 4 columns\n\n\n\nThis is of course much more effort than before. However, the data creation is clearly separated from the visualization. Let’s make use of this effort in the visualization.\n\nalt.Chart(df).mark_line().encode(\n    x=\"x\", y=\"y\", color=\"Frequency:N\", strokeDash=\"Function\", row=\"Function\"\n).properties(width=500)\n\n\n\n\n\n\n\nA couple of things are going on here. First we split up the plot into two subplots by specifying row=\"Function\", i.e. the column of the dataframe that should be used as a row identifier. Then we said color=\"Frequency:N\". Note the :N here. Without specifying that our data is nominal, it would be considered quantitative (:Q, the default), and the color map used for plotting would be a sequential color map instead of qualitative one.\nNow the great thing is that we can simply change what we want to color or arrange our plot by.\n\n(\n    alt.Chart(df)\n    .mark_line()\n    .encode(\n        x=\"x\",\n        y=\"y\",\n        color=\"Function:N\",\n        strokeDash=\"Frequency\",\n        row=\"Frequency\",\n    )\n    .properties(width=450, height=100)\n)\n\n\n\n\n\n\n\nI think this is quite a powerful framework and libraries like altair,seaborn,ggplot2,Plot fundamentally rely on this."
  },
  {
    "objectID": "contents/posts/grammar-of-graphics/main.html#a-gentle-introduction",
    "href": "contents/posts/grammar-of-graphics/main.html#a-gentle-introduction",
    "title": "Grammar of Graphics in Mathematics",
    "section": "",
    "text": "Say you’re a mathematician and you want to plot \\(sin(x)\\) and \\(sin(x)\\), how would you do that? You’d probably do something like this1:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots()\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), label=\"sin(x)\")\nax.plot(x, np.cos(x), label=\"cos(x)\")\nax.legend()\nax.set(title=\"Trigonometric functions\")\nplt.show()\n\n\n\n\n\n\n\n\nWhat I want to show you in this post is that it’s worth do alter that approach slightly. Let me show you.\n\nimport altair as alt\nimport pandas as pd\n\ndf = pd.concat(\n    [\n        pd.DataFrame({\"x\": x, \"y\": np.sin(x), \"Function\": \"sin\"}),\n        pd.DataFrame({\"x\": x, \"y\": np.cos(x), \"Function\": \"cos\"}),\n    ]\n)\nalt.Chart(df, title=\"Trigonometric functions\").mark_line().encode(\n    x=\"x\",\n    y=\"y\",\n    color=\"Function\",\n)\n\n\n\n\n\n\n\nLet’s unwrap what happens here. As a first note, we are using altair as our plotting library, but something like seaborn,ggplot2, or Plot would have been fine as well.\nNext, we are creating a pandas.DataFrame which is nothing but a table:\n\n\nCode\ndf\n\n\n\n\n\n\n\n\n\nx\ny\nFunction\n\n\n\n\n0\n0.00000\n0.000000\nsin\n\n\n1\n0.10101\n0.100838\nsin\n\n\n2\n0.20202\n0.200649\nsin\n\n\n3\n0.30303\n0.298414\nsin\n\n\n4\n0.40404\n0.393137\nsin\n\n\n...\n...\n...\n...\n\n\n95\n9.59596\n-0.985384\ncos\n\n\n96\n9.69697\n-0.963184\ncos\n\n\n97\n9.79798\n-0.931165\ncos\n\n\n98\n9.89899\n-0.889653\ncos\n\n\n99\n10.00000\n-0.839072\ncos\n\n\n\n\n200 rows × 3 columns\n\n\n\nThe one special thing about this dataframe however, is also the reason for this post. This is a long dataframe, not a wide dataframe. A wide dataframe would look like this:\n\n\nCode\n(\n    df.pivot(index=\"x\", columns=\"Function\", values=\"y\")\n    .reset_index()\n    .rename_axis(None, axis=1)\n)\n\n\n\n\n\n\n\n\n\nx\ncos\nsin\n\n\n\n\n0\n0.00000\n1.000000\n0.000000\n\n\n1\n0.10101\n0.994903\n0.100838\n\n\n2\n0.20202\n0.979663\n0.200649\n\n\n3\n0.30303\n0.954437\n0.298414\n\n\n4\n0.40404\n0.919480\n0.393137\n\n\n...\n...\n...\n...\n\n\n95\n9.59596\n-0.985384\n-0.170347\n\n\n96\n9.69697\n-0.963184\n-0.268843\n\n\n97\n9.79798\n-0.931165\n-0.364599\n\n\n98\n9.89899\n-0.889653\n-0.456637\n\n\n99\n10.00000\n-0.839072\n-0.544021\n\n\n\n\n100 rows × 3 columns\n\n\n\nThis dataframe is wide, because instead of stacking the values for \\(sin\\) and \\(cos\\) on top of another, they are side by side.\nThere are a couple of reasons why the long way is better than the wide way.\n\nA lot of modern visualization tools make heavy use of the Grammar of Graphics2 , an approach that is based on the long format.\nYou can store time series of different lengths in the same dataframe.\nA lot of data transformations (e.g. groupby) are much easier to use this way.\n\nSomething I very much thought of as a potential downside, however, is the different storage that is needed. The wide dataframe needs to store \\(3\\cdot N\\) double values (\\(x\\),\\(sin\\), \\(cos\\)), whereas the long format requires storage for \\(4\\cdot N\\) double values and \\(2\\cdot N\\) string values.\nAnother drawback is the added complexity when thinking about how the data should be stored.\nSo before I try to justify why this extra memory usage and complexity might be justified, let’s extend our example a little to make it slightly more complex. Let’s say we want to compare different frequencies for \\(sin\\) and \\(cos\\).\nWith our initial approach, this could look like this\n\nfig, ax = plt.subplots()\nx = np.linspace(0, 10, 100)\nfor i in range(1, 4):\n    ax.plot(x, np.sin(i * x), label=f\"sin({i}*x)\")\n    ax.plot(x, np.cos(i * x), label=f\"cos({i}*x)\")\nax.legend()\nax.set(title=\"Trigonometric functions\")\nplt.show()\n\n\n\n\n\n\n\n\nNot pretty, but you get the idea.\nHere’s the approach using altair and a long dataframe. First let’s bring the data in the correct form.\n\ndf = pd.concat(\n    [\n        pd.DataFrame(\n            {\n                \"x\": x,\n                \"y\": np.sin(i * x),\n                \"Function\": \"sin\",\n                \"Frequency\": i,\n            }\n        )\n        for i in range(1, 4)\n    ]\n    + [\n        pd.DataFrame(\n            {\n                \"x\": x,\n                \"y\": np.cos(i * x),\n                \"Function\": \"cos\",\n                \"Frequency\": i,\n            }\n        )\n        for i in range(1, 4)\n    ]\n)\ndf\n\n\n\n\n\n\n\n\nx\ny\nFunction\nFrequency\n\n\n\n\n0\n0.00000\n0.000000\nsin\n1\n\n\n1\n0.10101\n0.100838\nsin\n1\n\n\n2\n0.20202\n0.200649\nsin\n1\n\n\n3\n0.30303\n0.298414\nsin\n1\n\n\n4\n0.40404\n0.393137\nsin\n1\n\n\n...\n...\n...\n...\n...\n\n\n95\n9.59596\n-0.871008\ncos\n3\n\n\n96\n9.69697\n-0.684721\ncos\n3\n\n\n97\n9.79798\n-0.436037\ncos\n3\n\n\n98\n9.89899\n-0.147619\ncos\n3\n\n\n99\n10.00000\n0.154251\ncos\n3\n\n\n\n\n600 rows × 4 columns\n\n\n\nThis is of course much more effort than before. However, the data creation is clearly separated from the visualization. Let’s make use of this effort in the visualization.\n\nalt.Chart(df).mark_line().encode(\n    x=\"x\", y=\"y\", color=\"Frequency:N\", strokeDash=\"Function\", row=\"Function\"\n).properties(width=500)\n\n\n\n\n\n\n\nA couple of things are going on here. First we split up the plot into two subplots by specifying row=\"Function\", i.e. the column of the dataframe that should be used as a row identifier. Then we said color=\"Frequency:N\". Note the :N here. Without specifying that our data is nominal, it would be considered quantitative (:Q, the default), and the color map used for plotting would be a sequential color map instead of qualitative one.\nNow the great thing is that we can simply change what we want to color or arrange our plot by.\n\n(\n    alt.Chart(df)\n    .mark_line()\n    .encode(\n        x=\"x\",\n        y=\"y\",\n        color=\"Function:N\",\n        strokeDash=\"Frequency\",\n        row=\"Frequency\",\n    )\n    .properties(width=450, height=100)\n)\n\n\n\n\n\n\n\nI think this is quite a powerful framework and libraries like altair,seaborn,ggplot2,Plot fundamentally rely on this."
  },
  {
    "objectID": "contents/posts/grammar-of-graphics/main.html#more-complex-examples",
    "href": "contents/posts/grammar-of-graphics/main.html#more-complex-examples",
    "title": "Grammar of Graphics in Mathematics",
    "section": "More complex examples",
    "text": "More complex examples\nLet’s try to reproduce a figure from a paper I co-authored. The exact data does not really matter, but here’s what we want to end up with.\n\n\n\nA figure from a paper that I co-authored, https://arxiv.org/pdf/1808.05846.pdf\n\n\nNow, because I don’t have access to the real data anymore, we’ll use some fake data instead.\n\n\nCode\ndef get_fake_data(n, alpha) -&gt; pd.DataFrame:\n    nx = 10\n    x = np.linspace(0, 1, nx)\n\n    return pd.DataFrame(\n        {\n            \"x\": np.hstack([x, x, x]),\n            \"y\": np.hstack(\n                [\n                    np.exp(-x * n) * np.sin(alpha * x),\n                    np.exp(-x * n) * np.cos(alpha * x),\n                    np.exp(-x * n) * np.cos(alpha * x) * np.sin(alpha * x),\n                ]\n            ),\n            \"cut\": [\"hori\"] * nx + [\"diag\"] * nx + [\"verti\"] * nx,\n            \"alpha\": [alpha] * (3 * nx),\n            \"n\": [n] * (3 * nx),\n        }\n    )\n\n\nN = [1, 2, 3, 4]\nALPHA = [1, 2, 3, 4]\ndf = pd.concat([get_fake_data(n, alpha) for n in N for alpha in ALPHA])\n\n\n\ndf\n\n\n\n\n\n\n\n\nx\ny\ncut\nalpha\nn\n\n\n\n\n0\n0.000000\n0.000000\nhori\n1\n1\n\n\n1\n0.111111\n0.099222\nhori\n1\n1\n\n\n2\n0.222222\n0.176481\nhori\n1\n1\n\n\n3\n0.333333\n0.234445\nhori\n1\n1\n\n\n4\n0.444444\n0.275680\nhori\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n25\n0.555556\n-0.052251\nverti\n4\n4\n\n\n26\n0.666667\n-0.028256\nverti\n4\n4\n\n\n27\n0.777778\n-0.001357\nverti\n4\n4\n\n\n28\n0.888889\n0.010520\nverti\n4\n4\n\n\n29\n1.000000\n0.009060\nverti\n4\n4\n\n\n\n\n480 rows × 5 columns\n\n\n\nLet’s plot this data to recreate the original figure.\n\nalt.Chart(df).mark_line().encode(\n    x=\"x\",\n    y=\"y\",\n    row=\"n\",\n    column=\"alpha\",\n    color=\"cut\",\n).properties(width=100, height=100)\n\n\n\n\n\n\n\nNot identical, but you get the idea. The main issue here is the lack of LaTeX support. However, this is altair specific.\nLastly, not that there is nothing stopping us from having a nicer looking plot by simply changing the theme.\n\nfrom camminapy.plot import altair_theme\n\naltair_theme()\nalt.Chart(df).mark_line().encode(\n    x=\"x\",\n    y=\"y\",\n    row=\"n\",\n    column=\"alpha\",\n    color=\"cut\",\n).properties(width=100, height=100)"
  },
  {
    "objectID": "contents/posts/grammar-of-graphics/main.html#footnotes",
    "href": "contents/posts/grammar-of-graphics/main.html#footnotes",
    "title": "Grammar of Graphics in Mathematics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re using matlab instead of python or julia it might be worth considering a switch.↩︎\nhttps://link.springer.com/book/10.1007/0-387-28695-0↩︎"
  },
  {
    "objectID": "contents/posts/interactive-regression/interactive_regression.html",
    "href": "contents/posts/interactive-regression/interactive_regression.html",
    "title": "Interactive regression",
    "section": "",
    "text": "This example shows how to create an interactive plot with regression lines using altair. The regression lines will be computed over the window that is selected and update accordingly when moving the selected region.\n\n\nThis code block contains all import statements.\nimport polars as pl\nimport numpy as np\nimport altair as alt\nfrom vega_datasets import data\n\n\nLet’s look at a sample data set from vega_datasets, inspired by an example from the altair documentation.\n\nsource = data.iris()\nchart = (\n    alt.Chart(source)\n    .mark_circle()\n    .encode(\n        alt.X(\"sepalLength\").scale(zero=False),\n        alt.Y(\"sepalWidth\").scale(zero=False, padding=1),\n        color=\"species\",\n    )\n    .properties(width=600)\n)\nchart\n\n\n\n\n\n\n\nNow we can add a linear regression line very easily\n\nregression = chart.transform_regression(\n    \"sepalLength\",\n    \"sepalWidth\",\n    groupby=[\"species\"],\n    method=\"poly\",\n    order=5,\n).mark_line()\nalt.layer(chart, regression)\n\n\n\n\n\n\n\nNow let’s allow for some interactivity. We want to be able to select points and have the regression line be updated on that selection of points. However, we still want to plot it over the full domain, that’s why we have to set extent. We’ll make the stroke of the extrapolation range dashed and the interpolation range solid.\n\nbrush = alt.selection_interval()\n\n\nchart = (\n    alt.Chart(source)\n    .mark_circle()\n    .encode(\n        alt.X(\"sepalLength\").scale(domain=(4, 8)),\n        alt.Y(\"sepalWidth\").scale(domain=(1, 6)),\n        color=\"species:N\",\n    )\n    .properties(width=600)\n    .add_params(brush)\n)\n\nregression_solid = (\n    chart.transform_filter(brush)\n    .transform_regression(\n        \"sepalLength\",\n        \"sepalWidth\",\n        groupby=[\"species\"],\n        method=\"poly\",\n        order=3,\n    )\n    .mark_line(clip=True)\n)\n\nregression_dash = (\n    chart.transform_filter(brush)\n    .transform_regression(\n        \"sepalLength\",\n        \"sepalWidth\",\n        groupby=[\"species\"],\n        method=\"poly\",\n        order=3,\n        extent=[4, 8],\n    )\n    .mark_line(clip=True, strokeDash=[5, 5])\n)\n\n\nalt.layer(\n    chart.encode(\n        opacity=alt.condition(brush, alt.value(1.0), alt.value(0.1)),\n    ),\n    regression_dash,\n    regression_solid,\n)"
  },
  {
    "objectID": "contents/posts/nutri-score/main.html",
    "href": "contents/posts/nutri-score/main.html",
    "title": "Nutriscore analysis",
    "section": "",
    "text": "Code\n# %load_ext autoreload\n# %autoreload 2\n\nimport collections\nimport os\n\nimport altair as alt\nimport numpy as np\nimport polars as pl\nfrom camminapy.plot import altair_setup, altair_theme\nfrom rich import print\nfrom sklearn import tree\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\n\nfrom blog import logger\nfrom blog.io import (\n    ConvertDatetimeStrings,\n    ConvertNutrientsToFloats,\n    ConvertNutriScoresToCapitalLetters,\n    FilterGermany,\n    HasNutriScore,\n    KeepOnlyEnglishVersion,\n    KeepOnlyTagsVersion,\n    ValidCode,\n)\n\nalt.data_transformers.disable_max_rows()\naltair_theme()\n# altair_setup()\nlogger.setLevel(\"ERROR\")\n\nload_from_disk = True\nfinal_df_path = (\n    \"/Users/thomascamminady/Repos/blog/contents/posts/nutri-score/df.parquet\"\n)\nif load_from_disk and os.path.exists(final_df_path):\n    df = pl.read_parquet(final_df_path)\nelse:\n    path_csv = \"en.openfoodfacts.org.products.csv\"\n    path_parquet = path_csv.replace(\".csv\", \".parquet\")\n    if os.path.exists(path_parquet):\n        logger.info(\"Reading from .parquet file\")\n        _df = pl.read_parquet(\"en.openfoodfacts.org.products.parquet\")\n    else:\n        logger.info(\"Reading from .csv file.\")\n        _df = pl.read_csv(path_csv, separator=\"\\t\", ignore_errors=True)\n        _df.write_parquet(path_parquet)\n\n    df = (\n        _df.pipe(FilterGermany)  # We focus on products sold in Germany.\n        .pipe(HasNutriScore)  # We want products that have a non-null Nutriscore.\n        .pipe(ConvertNutrientsToFloats)  # Change dtype of columns with nutrients.\n        .pipe(ConvertDatetimeStrings)  # Change dtype to be Datetime.\n        .pipe(KeepOnlyEnglishVersion)  # Some duplicate columns, remove unwanted.\n        .pipe(KeepOnlyTagsVersion)  # Some duplicate columns, remove unwanted.\n        .pipe(ValidCode)  # Product code needs to be non-null and unique.\n        .pipe(ConvertNutriScoresToCapitalLetters)\n    )\n    df.write_parquet(final_df_path)\n\nn_non_nutrients = len([c for c in df.columns if not c.endswith(\"_100g\")])\nn_nutrients = len([c for c in df.columns if c.endswith(\"_100g\")])\nlogger.info(f\"\"\"Number of columns excluding nutrients: {n_non_nutrients}\"\"\")\nlogger.info(f\"\"\"Number of nutrients columns: {n_nutrients}\"\"\")\n\n\n\nThe Data\nThe data is taken from Open Food Facts where it is available for download under the Open Database License.\nI downloaded en.openfoodfacts.org.products.csv which is and 8.2GB file, containing 2.9 million products with 203 attributes stored per product, such as name, origin, sugar per 100g, etc.\nTo be able to work with this data, a couple of pre-processing steps and selections are executed which we will layout here with justifications if necessary:\n\nProducts must be sold in Germany. (I am German and I wanted to at least know some of the products. It also made my life easier because it reduces the amount of data by roughly a factor of 10.)\nProducts must have a valid Nutriscore. (The whole point of this analysis is to have a look at the Nutriscore, so items with a null Nutriscore are discarded. This reduces the data by another 3x.)\nRemove redundant information. (Some columns are redundant as they contain the same content, just in a different format. This drops about 25 columns.)\nRemove products with non-unique IDs. (This drops another couple of rows.)\n\nExecuting all these steps, we are left with around 73k products and 175 columns corresponding to different data fields. Out of those 175 columns, 118 contain information about the nutrients (per 100g).\nHere’s how the number of products split up across the different Nutriscores.\n\n\nCode\ndf.groupby(\"nutriscore_grade\", maintain_order=True).count().sort(\"nutriscore_grade\")\n\n\nIt is important to note, that products are listed in the data base multiple times, often because they occur in different portion sizes. For example, here are all products that are named “Snickers”.\n\n\nCode\ndf.filter(pl.col(\"product_name\") == \"Snickers\").select(\"product_name\", \"quantity\")\n\n\n\n\nNutriscore 101\nThe Nutriscore shall serve as a tool that allows to quickly compare products. The score can be one of “A”, “B”, “C”, “D”, or “E”. However, there is also an actual numeric score underpinning the grade.\n(Note that while the official Nutriscore has a green-to-red scale, we are using a blue-to-red scale to ensure better readability for people with colorblindness.)\nThis is shown in the next graph which presents the histogram of these numeric scores, colored by their Nutriscore grade.\n\n\nCode\nc = \"nutriscore_grade\"\ncolor = (\n    alt.Color(f\"{c}:N\")\n    .scale(\n        zero=False,\n        domain=[\"A\", \"B\", \"C\", \"D\", \"E\"],\n        # scheme=\"darkmulti\",\n        range=[\"blue\", \"lightblue\", \"gold\", \"orange\", \"red\"],\n    )\n    .legend(columns=1, symbolLimit=0, labelLimit=0)\n)\n\nalt.Chart(\n    df.select(\"nutriscore_grade\", \"nutriscore_score\")  # .sample(5_000)\n).mark_bar().encode(\n    x=alt.X(\"nutriscore_score:Q\").bin(step=1),\n    y=alt.Y(\"count():Q\"),\n    color=color,\n    row=\"nutriscore_grade:N\",\n).properties(\n    height=150, width=700\n).resolve_scale(\n    y=\"independent\"\n)\n\n\nThis mostly agrees with what I could find online\n\n\n\nNutriscore.jpeg\n\n\nInterestingly, there are a number of products that have an “E” grade but a lower numeric Nutriscore. Looking at a random selection of those, it seems that these are mostly beverages containing sugar.\n\n\nCode\nprint(\n    df.filter(pl.col(\"nutriscore_grade\") == \"E\")\n    .filter(pl.col(\"nutriscore_score\") &lt; 18)\n    .select(\"product_name\", \"categories_en\")\n    .sort(\"categories_en\")\n    .sample(n=20)[\"product_name\"]\n    .to_list()\n)\n\n\nNext, we can have a look at some of the nutrients. Let’s plot fat per 100g against sugar per 100g and group everything by Nutriscore.\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n\n\nchart = (\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").is_null().is_not()).select(x, y, c)\n        # .sample(5_000)\n    )\n    .mark_point(clip=True, filled=True, opacity=0.2)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n        tooltip=[\"product_name:N\"],\n    )\n    .properties(width=220, height=300)\n    .facet(facet=f\"{c}:N\", columns=3)\n)\n\nchart\n\n\nUnsuprisingly, “D” and “E” grade products seem to contain more sugar and fat.\nLet’s look at a histogram of the contained calories (per 100g) next.\n\n\nCode\nalt.Chart(\n    df.select(\"nutriscore_grade\", \"nutriscore_score\", \"energy-kcal_100g\").filter(\n        pl.col(\"energy-kcal_100g\") &lt; 1000\n    )  # .sample(5_000)\n).mark_bar().encode(\n    x=alt.X(\"energy-kcal_100g:Q\").bin(step=10),\n    y=alt.Y(\"count():Q\"),\n    color=color,\n    row=\"nutriscore_grade:N\",\n).properties(\n    height=150, width=700\n).resolve_scale(\n    y=\"independent\"\n)\n\n\nInterestingly, there is a spike at around 300kcal for the “A” grade foods, let’s look at what they are.\n\n\nCode\ndf.filter(pl.col(\"nutriscore_grade\") == \"A\").filter(\n    pl.col(\"energy-kcal_100g\").is_between(300, 350)\n).select(\"product_name\", \"categories_en\").sample(20)[\"product_name\"].to_list()\n\n\nThis is containing a lot of pasta, cereal and rice, but also flour.\nLet’s have a look at the top ten healthiest foods, according to their Nutriscore.\n\n\nCode\ndf.sort(\"nutriscore_score\").head(10)[\"product_name\"].to_list()\n\n\nAnd of course the ten least healthy foods.\n\n\nCode\ndf.sort(\"nutriscore_score\").tail(10)[\"product_name\"].to_list()\n\n\nThis also makes sense I guess. So far so good.\n\n\nGreenwashing?\nVery frequently I can’t believe how a specific product got an “A” grade, like how does some chocolate get an “A” grade even though it is high of sugar. Is there any way that companies are cheating the system?\nLet’s have a look at all the products that contain the word “choco” and see if we find something.\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").str.to_lowercase().str.contains(\"choco\"))\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=300, height=300)\n)\n\n\nHmm so this seems to suggest that “A” grades are actually properly assigned when sugar and fat content is low.\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").str.to_lowercase().str.contains(\"yoghurt\"))\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=300, height=300)\n)\n\n\nHow about “sweet snacks” (a label in the data frame) in general?\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(\n            pl.col(\"categories_en\").str.to_lowercase().str.contains(\"sweet snacks\")\n        )\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True, opacity=0.2)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=300, height=300)\n)\n\n\nWhat about vegan labels?\n\n\nCode\nx = \"sugars_100g\"\ny = \"fat_100g\"\n(\n    alt.Chart(\n        df.filter(\n            pl.col(\"ingredients_analysis_tags\").str.to_lowercase().str.contains(\"vegan\")\n        )\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=200, height=300)\n    .facet(facet=\"nutriscore_grade:N\", columns=3)\n)\n\n\nNext is protein vs. sugar.\n\n\nCode\nx = \"sugars_100g\"\ny = \"proteins_100g\"\n(\n    alt.Chart(\n        df.filter(pl.col(\"product_name\").str.to_lowercase().str.contains(\"choco\"))\n        .sort(\"nutriscore_grade\")\n        .select(x, y, \"nutriscore_grade\")\n    )\n    .mark_point(filled=True, clip=True)\n    .encode(\n        x=alt.X(f\"{x}:Q\").scale(domain=(0, 100)),\n        y=alt.Y(f\"{y}:Q\").scale(domain=(0, 100)),\n        color=color,\n    )\n    .properties(width=200, height=300)\n    .facet(facet=\"nutriscore_grade:N\", columns=3)\n)\n\n\nSo no greenwashing? Need to continue this.\n\n\nCorrelations\nLet’s see which of the nutrients correlate with the Nutriscore.\nTo do that, we pick all columns that contain ingredients about the nutrients and where at most 10% of the entries are null.\n\n\nCode\nd = (\n    (\n        df.select([c for c in df.columns if c.endswith(\"_100g\")]).null_count()\n        / len(df)\n        * 100\n    )\n    &lt;= 10\n)[0].to_dict()\ncolumns = [\"nutriscore_score\"] + [key for key, values in d.items() if values[0]]\n\ndisplay(\n    df.select(columns)\n    .to_pandas()\n    .corr()[[\"nutriscore_score\"]]\n    .iloc[1:-1, :]\n    .sort_values(\"nutriscore_score\")\n    .style.background_gradient(cmap=\"RdBu\", vmin=-1, vmax=1)\n)\n\n\nSo (saturated) fat is the best indicator for a Nutriscore.\nThe obvious next question is, how well can we predict this. We’ll use simple Decision Trees.\n\n\nCode\ncolumns_for_fitting = [\n    \"energy-kcal_100g\",\n    \"energy_100g\",\n    \"fat_100g\",\n    \"saturated-fat_100g\",\n    \"carbohydrates_100g\",\n    \"sugars_100g\",\n    \"proteins_100g\",\n    \"salt_100g\",\n    \"sodium_100g\",\n    # \"nutrition-score-fr_100g\",\n]\n\n# columns_for_fitting =columns\n\n\n\n\nCode\ny = df.select(\"nutriscore_score\").to_numpy().flatten()\nlogger.info(y.shape)\n\nX = df.select(\n    [\n        c\n        for c in columns_for_fitting\n        if (c != \"nutriscore_score\" and c != \"nutrition-score-fr_100g\")\n    ]\n).to_numpy()\nlogger.info(X.shape)\n\n\nlogger.info(\"Keep only data that has no nans.\")\n\nselect = np.sum(np.isnan(X), axis=1) == 0\ny = y[select]\nX = X[select, :]\nlogger.info(y.shape)\nlogger.info(X.shape)\n\n\ntransformer = Normalizer().fit(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    transformer.transform(X), y, test_size=0.20, random_state=2023\n)\n\n\nclf = tree.DecisionTreeRegressor(max_depth=15)\nclf = clf.fit(X_train, y_train)\n\n\ndf_tree = pl.concat(\n    [\n        pl.DataFrame(\n            {\n                \"actual score\": y_test,\n                \"predicted score\": clf.predict(X_test),\n                \"label\": \"test\",\n            }\n        ),\n        pl.DataFrame(\n            {\n                \"actual score\": y_train,\n                \"predicted score\": clf.predict(X_train),\n                \"label\": \"train\",\n            }\n        ),\n    ]\n).with_columns(err=pl.col(\"predicted score\") - pl.col(\"actual score\"))\n\n\nchart_v1 = (\n    alt.Chart(df_tree)\n    .mark_point(filled=True, opacity=0.02)\n    .encode(\n        x=\"actual score:Q\",\n        y=\"predicted score:Q\",\n        # y=\"err:Q\",\n        color=\"label:N\",\n        column=\"label:N\",\n    )\n    .properties(width=300, height=300)\n)\n\ndisplay(chart_v1)\n\n\nA little underwhelming. Maybe adding information on the categories helps. Let’s find the top 20 labels and one-hot-encode them.\n\n\nCode\nlol_categories = (\n    df.select(\"categories_en\")\n    .with_columns(pl.col(\"categories_en\").str.split(\",\"))[\"categories_en\"]\n    .to_list()\n)\n\n\ncategories = []\nfor l in lol_categories[:100]:\n    categories.extend(l)\n\n\nhist = dict(collections.Counter(categories))\n\ndf_hist = (\n    pl.DataFrame(\n        {\n            \"categorie\": [key for key, _ in hist.items()],\n            \"count\": [value for _, value in hist.items()],\n        }\n    )\n    .sort(\"count\", descending=True)\n    .head(20)\n)\nprint(df_hist[\"categorie\"].to_list())\n\n\nAlright so we can now one-hot-encode these. Let’s run the classifier again, but now we add these labels. We can also check the correlation again.\n\n\nCode\ndf_one_hot = df.with_columns(\n    (pl.col(\"categories_en\").str.to_lowercase().str.contains(x.lower()))\n    .cast(int)\n    .alias(f\"1hot__{x}\")\n    for x in df_hist[\"categorie\"].to_list()\n)\ncolumns_one_hot = [c for c in df_one_hot.columns if \"1hot\" in c]\nlogger.info(columns_one_hot)\n\ny = df_one_hot.select(\"nutriscore_score\").to_numpy().flatten()\nlogger.info(y.shape)\n\nX = df_one_hot.select(\n    [\n        c\n        for c in columns_for_fitting + columns_one_hot\n        if (c != \"nutriscore_score\" and c != \"nutrition-score-fr_100g\")\n    ]\n).to_numpy()\nlogger.info(X.shape)\n\n\nlogger.info(\"Keep only data that has no nans.\")\n\nselect = np.sum(np.isnan(X), axis=1) == 0\ny = y[select]\nX = X[select, :]\nlogger.info(y.shape)\nlogger.info(X.shape)\n\nd = (\n    (\n        df_one_hot.select(\n            [c for c in df_one_hot.columns if c.endswith(\"_100g\")]\n        ).null_count()\n        / len(df)\n        * 100\n    )\n    &lt;= 10\n)[0].to_dict()\n\ncolumns = (\n    [\"nutriscore_score\"]\n    + [key for key, values in d.items() if values[0]]\n    + columns_one_hot\n)\n\ndisplay(\n    df_one_hot.select(columns)\n    .to_pandas()\n    .corr()[[\"nutriscore_score\"]]\n    .iloc[1:-1, :]\n    .sort_values(\"nutriscore_score\")\n    .style.background_gradient(cmap=\"RdBu\", vmin=-1, vmax=1)\n)\n\n\nlogger.info(X.shape)\ntransformer = Normalizer().fit(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    transformer.transform(X), y, test_size=0.20, random_state=2023\n)\n\nclf = tree.DecisionTreeRegressor(max_depth=15)\nclf = clf.fit(X_train, y_train)\n\n\ndf_tree_one_hot = pl.concat(\n    [\n        pl.DataFrame(\n            {\n                \"actual score\": y_test,\n                \"predicted score\": clf.predict(X_test),\n                \"label\": \"test\",\n            }\n        ),\n        pl.DataFrame(\n            {\n                \"actual score\": y_train,\n                \"predicted score\": clf.predict(X_train),\n                \"label\": \"train\",\n            }\n        ),\n    ]\n).with_columns(err=pl.col(\"predicted score\") - pl.col(\"actual score\"))\n\n\nnew_chart = (\n    alt.Chart(df_tree_one_hot)\n    .mark_point(filled=True, opacity=0.02)\n    .encode(\n        x=\"actual score:Q\",\n        y=\"predicted score:Q\",\n        # y=\"err:Q\",\n        color=\"label:N\",\n        column=\"label:N\",\n    )\n    .properties(width=300, height=300)\n)\n\n\nchart_v1 & new_chart\n\n\n\n\nCode\ndf_tree_full = pl.concat(\n    [\n        df_tree.with_columns(pl.lit(\"simple\").alias(\"model\")),\n        df_tree_one_hot.with_columns(pl.lit(\"one_hot\").alias(\"model\")),\n    ]\n)\n\n\n\n\nCode\nalt.Chart(df_tree_full).mark_bar().encode(\n    x=alt.X(\"err:Q\").bin(step=2).axis(values=np.arange(-30, 35, 5)),\n    y=alt.Y(\"count():Q\"),\n    color=\"label:N\",\n    row=\"model:N\",\n    column=\"label:N\",\n).properties(width=300, height=200).resolve_scale(y=\"independent\")\n\n\n\n\nCode\ndf_tree_full.groupby(\"label\", \"model\").agg(\n    err_min=pl.col(\"err\").min(),\n    err_mean=pl.col(\"err\").mean(),\n    err_median=pl.col(\"err\").median(),\n    err_std=pl.col(\"err\").std(),\n    err_max=pl.col(\"err\").max(),\n)\n\n\nThis looks fine for now, with the one-hot-encoded model, the error is mostly acceptable.\nHowever, let’s try one more time, this time with classification instead of regression.\n\n\nCode\ndf_one_hot = df.with_columns(\n    (pl.col(\"categories_en\").str.to_lowercase().str.contains(x.lower()))\n    .cast(int)\n    .alias(f\"1hot__{x}\")\n    for x in df_hist[\"categorie\"].to_list()\n)\ncolumns_one_hot = [c for c in df_one_hot.columns if \"1hot\" in c]\n\ny = df_one_hot.select(\"nutriscore_grade\").to_numpy().flatten()\n\nX = df_one_hot.select(\n    [\n        c\n        for c in columns_for_fitting + columns_one_hot\n        if (c != \"nutriscore_score\" and c != \"nutrition-score-fr_100g\")\n    ]\n).to_numpy()\n\n\nselect = np.sum(np.isnan(X), axis=1) == 0\ny = y[select]\nX = X[select, :]\nlogger.info(y.shape)\nlogger.info(X.shape)\n\n\nclf = tree.DecisionTreeClassifier(max_depth=10)\ntransformer = Normalizer().fit(X)\nX_train, X_test, y_train, y_test = train_test_split(\n    transformer.transform(X), y, test_size=0.80, random_state=2023\n)\nclf.fit(X_train, y_train)\n\ny_test_predict = clf.predict(X_test)\ny_train_predict = clf.predict(X_train)\n\nlogger.info(\"Training confusion matrix\")\n# cm = confusion_matrix(y_test, y_test_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\ncm = confusion_matrix(y_train, y_train_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\"\"\"Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\"\"\"\nletter = {4: \"A\", 3: \"B\", 2: \"C\", 1: \"D\", 0: \"E\"}\ndisplay(\n    pl.DataFrame(cm)\n    .rename(\n        mapping={\n            f\"column_{i}\": f\"predicts {letter}\"\n            for i, letter in zip([0, 1, 2, 3, 4], [\"A\", \"B\", \"C\", \"D\", \"E\"])\n        }\n    )\n    .with_columns(\n        pl.col(\"predicts A\")\n        .rank()\n        .cast(int)\n        .apply(lambda r: letter[r - 1])\n        .alias(\"true label\")\n    )\n    .select(\n        \"true label\",\n        \"predicts A\",\n        \"predicts B\",\n        \"predicts C\",\n        \"predicts D\",\n        \"predicts E\",\n    )\n    .to_pandas()\n    .style.background_gradient()\n)\n\n\nlogger.info(\"Testing confusion matrix\")\ncm = confusion_matrix(y_test, y_test_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n# cm = confusion_matrix(y_train, y_train_predict, labels=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\"\"\"Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and predicted label being j-th class.\"\"\"\nletter = {4: \"A\", 3: \"B\", 2: \"C\", 1: \"D\", 0: \"E\"}\ndisplay(\n    pl.DataFrame(cm)\n    .rename(\n        mapping={\n            f\"column_{i}\": f\"predicts {letter}\"\n            for i, letter in zip([0, 1, 2, 3, 4], [\"A\", \"B\", \"C\", \"D\", \"E\"])\n        }\n    )\n    .with_columns(\n        pl.col(\"predicts A\")\n        .rank()\n        .cast(int)\n        .apply(lambda r: letter[r - 1])\n        .alias(\"true label\")\n    )\n    .select(\n        \"true label\",\n        \"predicts A\",\n        \"predicts B\",\n        \"predicts C\",\n        \"predicts D\",\n        \"predicts E\",\n    )\n    .to_pandas()\n    .style.background_gradient()\n)\n\n\n\n\nSummary\nThis was a good first deep dive into the data. Nothing suspicous as of yet. No obvious green washing and prediction works reasonably well (as expected). To be continued.!"
  },
  {
    "objectID": "contents/cv.html",
    "href": "contents/cv.html",
    "title": "Experience",
    "section": "",
    "text": "Experience\nWahoo Fitness, Atlanta/remote, Algorithm Developer\nSince March 2021\n\nCreation of APIs that allow the automatic monitoring of hardware and software quality metrics.\nTime series analysis and sensor fusion for various sources of data using Python.\nStatistical analysis of sports related data and creation of interactive dashboards for visualization.\nCode generation for low-level hardware using Matlab.\nCode documentation and result summaries in Confluence.\nSprint planning using Jira in an agile environment.\n\nSteinbuch Centre for Computing, Karlsruhe, Scientific Staff\nSeptember 2017 - March 2021\n\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nUsing concepts from machine learning and optimization theory, to automatically generate near-optimal sets of problem-dependent numerical parameters for the solution of advection problems.\nI optimized the performance of research software on the HPC cluster of the Karlsruhe Institute of Technology via parameter studies and OpenMP.\nFor modules in the mathematics M.Sc. program (including Kinetic Theory and Uncertainty Quantification) and modules in the Computational Engineering Science B.Sc. program, I was a teaching assistant and substituted lectures.\n\nCenter for Computational Engineering Science, Aachen, Scientific Staff\nMarch 2015 - September 2017\n\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nHelp with lectures in the Computational Engineering Module.\nPresentations at international conferences.\n\n\n\nEducation\nKarlsruhe Institute of Technology, Ph.D. Applied Mathematics\nSeptember 2017 to March 2021\n\nDissertation: “Theory, models, and numerical methods for classical and non-classical transport”\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nUsing concepts from machine learning and optimization theory, to automatically generate near- optimal sets of problem-dependent numerical parameters for the solution of advection problems.\nI optimized the performance of research software on the HPC cluster of the Karlsruhe Institute of Technology via parameter studies and OpenMP.\nFor modules in the mathematics M.Sc. program (including Kinetic Theory and Uncertainty Quantification) and modules in the Computational Engineering Science B.Sc. program, I was a teaching assistant and substituted lectures.\n\nRWTH Aachen University, M.Sc. Computational Engineering Science\nSeptember 2013 to September 2015\n\nThesis: “Theory and application of numerical methods for fractional diffusion equations”\nTutoring in the field of numerical mathematics and programming.\nResearch student at the Mathematical Center for Computational Engineering Science.\nCourses on (applied) mathematics, optimization methods, and algorithmic differentiation.\n\nRWTH Aachen University, B.Sc. Computational Engineering Science\nOctober 2009 to September 2013\n\nThesis: “Improvement of the aerodynamic shape optimization by adjoint methods in an MDO process”\nInternship and Bachelor’s thesis with EADS Cassidian, Manching, Germany.\nCourses on (applied) mathematics, fluid mechanics, and high-performance computing.\n\n\n\nPublications\nJournal articles\n\nMathematische Grundlagen der künstlichen Intelligenz im Schulunterricht, Sarah Schönbrodt, TC, and Martin Frank. Mathematische Semesterberichte, pages 1–29, 2021.\nRay Effect Mitigation for the Discrete Ordinates Method Using Artificial Scattering, Martin Frank, Jonas Kusch, TC, and Cory D. Hauck. Nuclear Science and Engineering, 194(11):971–988, 2020.\nRay effect mitigation for the discrete ordinates method through quadrature rotation, TC, Martin Frank, Kerstin Küpper, and Jonas Kusch. Journal of Computational Physics, 382:105 – 123, 2019.\nA spectral galerkin method for the fractional order diffusion and wave equation, TC and Martin Frank. International Journal of Advances in Engineering Sciences and Applied Mathematics, 10(1): 90–104, 2018.\nA new high-order fluid solver for tokamak edge plasma transport simulations based on a magnetic-field independent discretization, G. Giorgiani, TC, H. Bufferand, G. Ciraolo, P. Ghendrih, H. Guillard, H. Heumann, B. Nkonga, F. Schwander, E. Serre, and P. Tamain. Contributions to Plasma Physics, 58(6-8):688–695, 2018.\n\nConference proceedings\n\nHighly uniform quadrature sets for the discrete ordinates method, TC, Martin Frank, and Jonas Kusch. In Proc. Int. Conf. Mathematics and Computational Methods Applied to Nuclear Science and Engineering, pages 25–29, 2019.\nNonclassical particle transport in heterogeneous materials, TC, Martin Frank, and Edward W Larsen. In International Conference on Mathematics & Computational Methods Applied to Nuclear Science & Engineering, 2017.\nThe equivalence of forward and backward nonclassical particle transport theories, Edward W Larsen, Martin Frank, and TC. In International Conference on Mathematics & Computational Methods Applied to Nuclear Science & Engineering, 2017.\n\nBook chapters\n\nVorschlag für eine Abiturprüfungsaufgabe mit authentischem und relevantem Realitätsbezug, Maike Sube, TC, Martin Frank, and Christina Roeckerath. Springer Berlin Heidelberg, Berlin, Heidelberg, 2020."
  },
  {
    "objectID": "contents/blog.html",
    "href": "contents/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Grammar of Graphics in Mathematics\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly creating heatmaps with pandas\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nNutriscore analysis\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nDas Verkehrsproblem in Balve lösen\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive regression\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nAbiturnoten inflation in Germany\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nTrack and Field World Record Progression\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nComparing polars with pandas; my personal case study\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n\n\n\n\n\n\nLogitech MX Mechanical Mini for Mac: Issues with M1 Mac\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\nNo matching items"
  }
]