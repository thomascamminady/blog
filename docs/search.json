[
  {
    "objectID": "posts/gradeinflation/Abiturnoten.html",
    "href": "posts/gradeinflation/Abiturnoten.html",
    "title": "Abiturnoten inflation in Germany",
    "section": "",
    "text": "Code\nfrom create_plot import run\n\nrun()\n\n\nThe German Abitur - colloquially known as the ‘Abitur’ - represents the culmination of a student’s secondary education. This advanced level of high school education concludes with an exam of significant importance. Successfully passing the Abitur opens the door to tertiary education institutions - universities, colleges, and certain professional schools. Given the weightage of this examination, an upward trend in the grades observed over the years deserves a closer inspection.\nRecently, an intriguing analysis was conducted, plotting the trajectory of Abitur grades over time. The resulting graph reveals a surprising trend: while there has been an increase in the number of good grades, the number of mediocre grades has largely remained unchanged, and bad grades have seen a decline. This pattern merits a deeper dive into the phenomenon of grade inflation, a subject that has been a topic of substantial debate in educational circles worldwide.\nGrade inflation refers to the trend where higher grades are awarded for the same quality of work compared to previous years. In essence, the standard of what constitutes a ‘good’ or ‘excellent’ grade is slowly diminished. This can lead to a devaluation of educational attainment, making it more challenging for universities and employers to discern the calibre of graduates. It also raises concerns about the efficacy of the education system itself: are students truly performing better, or are the standards being lowered?\nThe figure produced from the recent analysis offers visual evidence of grade inflation in the Abitur. The rising trend of good grades suggests that more and more students are performing at a level previously considered exceptional. On the surface, this could be interpreted as a positive development, signaling increased student achievement. However, the stagnation of mediocre grades and the decrease in poor grades provide a different perspective.\nWhy are mediocre grades stagnating, and why are bad grades dwindling? If the education system is improving uniformly, we would expect an overall shift with improvements across the board. The specificity of this trend suggests that the bar for what constitutes good performance might be lowering, thereby allowing more students to achieve ‘good’ grades.\nFurthermore, this pattern is potentially problematic for universities and employers who rely on these grades to gauge a student’s competence. If more students are receiving good grades, the differentiation between an average and an outstanding student becomes blurred, reducing the efficacy of grades as a tool for assessing student abilities.\nThis analysis is crucial in sparking further discussion on the phenomenon of grade inflation. It opens up a much-needed discourse on the current educational standards and how they are evolving. It also brings to light the importance of maintaining rigorous, consistent grading standards to ensure that grades remain a reliable measure of student achievement.\nThe journey to understand the German Abitur’s evolving landscape continues, and such investigations provide essential insights. They remind us that while grades are critical, they are just one aspect of education. The ultimate goal should always be the cultivation of knowledge, skills, and character, ensuring that students are truly prepared for the challenges they will face in their futures.\n\n\n\nGrade inflation German Abitur, share of 1.0.\n\n\n\n\n\nGrade inflation German Abitur"
  },
  {
    "objectID": "posts/logitech-issue/index.html",
    "href": "posts/logitech-issue/index.html",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "",
    "text": "About two month ago, I bought a Logitech MX Mechanical Mini for Mac and it worked like a charm with my previous Intel Macbook Pro (2019). Last week I received a new work Macbook that uses the newer M1 chip (M1 Max). Since then I am experiencing issues with the connectivity of the MX Mechanical Mini.\nAs of now (Feb 22, 2023) I have no solution for this issue."
  },
  {
    "objectID": "posts/logitech-issue/index.html#whats-the-issue",
    "href": "posts/logitech-issue/index.html#whats-the-issue",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "What’s the issue?",
    "text": "What’s the issue?\nIt’s a connectivity issue, where it seems like the keyboard goes to sleep after only a couple of seconds of inactivity (smaller than ten seconds). Waking up the keyboard then takes some three to four seconds, time during which no keystrokes are recorded. This is especially annoying for me, since I work as a developer and there are frequent periods of not typing (but thinking), followed by quick bursts of typing.\nI did not notice this issue on the Intel-based Macbook which makes me wonder whether this is an issue on the hardware side of things."
  },
  {
    "objectID": "posts/logitech-issue/index.html#what-have-i-tried",
    "href": "posts/logitech-issue/index.html#what-have-i-tried",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "What have I tried?",
    "text": "What have I tried?\nI tried the usual things: - Making sure that the battery is charged. - Updating driver (Logi Options+) - Disconnect the device, forget the device in bluetooth settings, and reconnect. - Factory reset the keyboard, including the removal of all previously paired devices. - Pairing my Mac on each of the 3 different Easy-Switch slots to see whether the slot makes a difference."
  },
  {
    "objectID": "posts/logitech-issue/index.html#references-ressources",
    "href": "posts/logitech-issue/index.html#references-ressources",
    "title": "Logitech MX Mechanical Mini for Mac: Issues with M1 Mac",
    "section": "References / Ressources",
    "text": "References / Ressources\nI looked for this issue and found some recent posts / threads on reddit: - MX mechanical sleeps - MX keys randomly disconnecting\nThis website explains how to factory reset your keyboard such that all previously paired devices are forgotten: - Factory reset Logitech MX Keys Keyboard (Note: The sequence is esc O esc O esc B, where O is the fifteenth letter of the alphabet, not the number 0.)"
  },
  {
    "objectID": "posts/world-records/records.html",
    "href": "posts/world-records/records.html",
    "title": "Track and Field World Record Progression",
    "section": "",
    "text": "Code\nimport altair as alt\nimport polars as pl\nfrom alltime_athletics_python.io import download_data\nfrom alltime_athletics_python.io import import_running_only_events\n\n# download_data()\ndf = import_running_only_events(\"./data\")\n\n\n\n\nCode\nworld_records = (\n    df.filter(pl.col(\"event\").str.contains(\"walk\") == False)\n    .filter(pl.col(\"event type\") == \"standard\")\n    .sort(\"sex\", \"distance\", \"event\", \"date of event\")\n    .with_columns(\n        pl.col(\"result seconds\")\n        .cummin()\n        .over(\"sex\", \"event\")\n        .alias(\"world record time\")\n    )\n    .filter(pl.col(\"result seconds\") == pl.col(\"world record time\"))\n    .groupby(\"sex\", \"event\", \"result seconds\", maintain_order=True)\n    .first()\n    .with_columns(\n        (\n            100\n            * pl.col(\"result seconds\")\n            / pl.col(\"result seconds\").min().over(\"sex\", \"event\")\n        ).alias(\"percent of wr\")\n    )\n)\n\nworld_records = pl.concat(\n    [\n        world_records,\n        world_records.filter(pl.col(\"rank\") == 1).with_columns(\n            [\n                pl.lit(\"2023-06-07\")\n                .str.strptime(pl.Date, format=\"%Y-%m-%d\")\n                .alias(\"date of event\"),\n                pl.lit(-1).cast(pl.Int64).alias(\"rank\"),\n            ]\n        ),\n    ]\n).with_columns(pl.col(\"sex\").apply(lambda s: s.title()))\n\ndata = world_records.select(\n    \"date of event\", \"percent of wr\", \"event\", \"sex\", \"name\", \"rank\"\n).to_pandas()\nlegend_selection = alt.selection_point(fields=[\"event\"], bind=\"legend\")\nlegend_selection_empty = alt.selection_point(\n    fields=[\"event\"], bind=\"legend\", empty=False\n)\n\nbase = (\n    alt.Chart(data)\n    .encode(\n        x=alt.X(\"date of event:T\")\n        .scale(domain=(\"1950-01-01\", \"2026-01-01\"))\n        .title(\"Year\"),\n        y=alt.Y(\"percent of wr:Q\")\n        .scale(domain=(100, 110))\n        .axis(values=list(range(100, 120, 2)))\n        .title(\"Time in % of current WR\"),\n        color=alt.Color(\n            \"event:N\",\n            sort=world_records.sort(\"distance\")[\"event\"]\n            .unique(maintain_order=True)\n            .to_list(),\n        )\n        .scale(scheme=\"dark2\")\n        .title(\"Event\"),\n        # strokeDash=\"sex:N\",\n        opacity=alt.condition(legend_selection, alt.value(1), alt.value(0)),\n    )\n    .properties(width=800, height=500)\n    .add_params(legend_selection)\n    .add_params(legend_selection_empty)\n)\n\nbase_no_endpoint = base.transform_filter(alt.datum[\"rank\"] &gt; 0)\n\ntext = base_no_endpoint.encode(\n    text=\"name:N\",\n    opacity=alt.condition(legend_selection_empty, alt.value(0.9), alt.value(0.0)),\n)\n\n\nchart = (\n    alt.layer(\n        base.mark_line(interpolate=\"step-after\", clip=True, strokeWidth=3),\n        base_no_endpoint.mark_point(filled=True, clip=True, size=100),\n        text.mark_text(clip=True, fontSize=14, angle=270 + 45, align=\"left\", dx=15),\n    )\n    .facet(\n        row=alt.Row(\"sex:N\").title(\"\").header(labelAngle=0),\n        title=\"World Record Progression\",\n    )\n    .resolve_scale(x=\"independent\")\n)\nchart\n\n\n\n\n\n\n\nFigure 1: Progression of world records."
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html",
    "href": "posts/polars-vs-pandas/index.html",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "",
    "text": "A couple of weeks ago I came across polars, a “Lightning-fast DataFrame library for Rust and Python.” Since then, I have been playing around with it, trying to do some of my daily data analyses tasks with polars instead of pandas.\nI wanted to summarize my experience using polars for some of the work that I am doing by comparing my polars implementation of a data analysis pipeline to the equivalent pipeline using pandas. The emphasis here is on the fact that it is my implementation. I am sure that both the polars and the pandas implementation can be improved or are not necessarily following best practices. Moreover, I am barely fluent in polars at this point.\nNevertheless, I think that I learned something for myself and have formed some opinion on things I like and dislike."
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html#preface",
    "href": "posts/polars-vs-pandas/index.html#preface",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "",
    "text": "A couple of weeks ago I came across polars, a “Lightning-fast DataFrame library for Rust and Python.” Since then, I have been playing around with it, trying to do some of my daily data analyses tasks with polars instead of pandas.\nI wanted to summarize my experience using polars for some of the work that I am doing by comparing my polars implementation of a data analysis pipeline to the equivalent pipeline using pandas. The emphasis here is on the fact that it is my implementation. I am sure that both the polars and the pandas implementation can be improved or are not necessarily following best practices. Moreover, I am barely fluent in polars at this point.\nNevertheless, I think that I learned something for myself and have formed some opinion on things I like and dislike."
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html#setup",
    "href": "posts/polars-vs-pandas/index.html#setup",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Setup",
    "text": "Setup\nI want to briefly discuss the data that I am encountering for this case study, as well as the steps in the analysis that I am performing.\nThe data I am using here is stored in a parquet file and the resulting data frame as approximately 40k rows with some 100 columns.\nIn a simplified way, that data looks like the following frame. There are two keys which contain measurements (thing of the first key of measurements with recording device A and B, and the second key of different days of the recordings.) Each measurement is a time series with columns t and time representing the local and global time, respectively. At those points in time, signals y1, y2, y3, … are recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkey1\nkey2\nt\ny1\ny2\ny3\ntime\n\n\n\n\n0\nA\nU\n0\n0.342872\n0.731905\n0.341766\n02/02/2023 15:07:21.68\n\n\n1\nA\nV\n1\n0.25941\n0.493496\n0.434559\n02/02/2023 15:07:21.88\n\n\n2\nA\nW\n2\n0.485956\n0.550383\n0.521913\n02/02/2023 15:07:22.28\n\n\n3\nA\nX\n3\n0.210544\n0.406669\n0.540021\n02/02/2023 15:07:22.58\n\n\n4\nB\nU\n2\n0.830654\n0.0386757\n0.635353\n02/02/2023 15:07:22.88\n\n\n5\nB\nV\n3\n0.187675\n0.919848\n0.648574\n02/02/2023 15:07:23.28\n\n\n6\nB\nW\n4\n0.506172\n0.93743\n0.554965\n02/02/2023 15:07:23.58\n\n\n7\nB\nX\n5\n0.21009\n0.829689\n0.857681\n02/02/2023 15:07:23.88\n\n\n\nThe code below obfuscates the real column names because I don’t want to give away sensitive information. However, It is worth outlining the steps that I am doing in the analysis. These steps include: - The conversion of time stamps to datetime formats. - Grouping data over keys and taking the mean value of the initial N samples of some columns. - Subtracting those means from different columns. - Computing rolling means of some columns and using those computed means to replace data in those rows where the mean is below a certain threshold. - Compute a bunch of derived columns that use one or more of the existing columns in some transformation. Those are row wise operations that require no grouping or anything fancy."
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html#using-polars",
    "href": "posts/polars-vs-pandas/index.html#using-polars",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Using polars",
    "text": "Using polars\nHere’s my implementation using polars for a total of 50 lines of code (LOC). I scan the data instead of reading it directly to run the whole pipeline in a lazy way. Only the call to collect at the end actually forces a computation. Internally, the operations can be optimized and made more efficient. I really like the chaining of operations. While it is somewhat verbose, it is consistent: Every new operation just gets chained to the existing operations with the .keyword() syntax. Computing averages over groups with the .median().over() syntax feels nicer than the pandas equivalent of .groupby().transform().\nCreating new columns with the .with_columns() syntax has the downside, that you need to chain multiple calls to .with_columns() after another if you want to access a column that was created in a prior computation. This is also the reason why my polars implementation has roughly twice the number of LOC when compared with the pandas implementation.\nOne downside that I saw is that, different from pandas, I do not get any kind of auto-complete for the columns that are in a data frame when using the pl.col(\"column name\") syntax. In pandas, VSCode will allow you to auto-complete the column name if you start typing df[\"column and column_name will pop up as a suggestion if it is an element of the data frame.\nlazy_frame = (\n    pl.scan_parquet(path)\n    .rename(mapping={\"column1\": \"criteria2\"})\n    .with_columns(\n        [\n            pl.col(\"time\").str.strptime(pl.Datetime, fmt=\"%m/%d/%Y %H:%M:%S%.f\"),\n            (pl.col(\"y1\") * 16.7).alias(\"z1\"),\n        ]\n    )\n    .with_columns(\n        [\n            pl.col(\"y2\").head(20).median().over(\"criteria1\", \"criteria2\").alias(\"z3\"),\n            pl.col(\"z1\").head(20).median().over(\"criteria1\", \"criteria2\").alias(\"z4\"),\n            pl.col(\"t\").min().over(\"criteria1\", \"criteria2\").alias(\"z5\"),\n        ]\n    )\n    .with_columns((pl.col(\"z4\") - pl.col(\"z3\")).alias(\"z6\"))\n    .with_columns(\n        [\n            (pl.col(\"z1\") - pl.col(\"z6\")).alias(\"y8\"),\n            (pl.col(\"y2\").rolling_min(8, center=True) &lt; 10).alias(\"z10\"),\n            (pl.col(\"t\") - pl.col(\"z5\")).alias(\"t\"),\n            pl.when(pl.col(\"y2\").rolling_min(8, center=True) &lt; 10)\n            .then(float(\"nan\"))\n            .otherwise(pl.col(\"y2\"))\n            .alias((\"z2\")),\n        ]\n    )\n    .with_columns(\n        [\n            (2 * 3.14159 / 60 * pl.col(\"y2\") * pl.col(\"y4\")).alias(\"z8\"),\n            (2 * 3.14159 / 60 * pl.col(\"z2\") * pl.col(\"y4\")).alias((\"z7\")),\n            (2 * 3.14159 / 60 * pl.col(\"y8\") * pl.col(\"y4\")).alias(\"y9\"),\n        ]\n    )\n    .with_columns((pl.col(\"y9\") - pl.col(\"z7\")).alias(\"Error\"))\n    .with_columns(\n        (100 * pl.col(\"Error\") / pl.col(\"z7\")).alias(\"Percentage Error\"),\n    )\n    .with_columns(((0.5 * pl.col(\"y10\") + 0.5 * pl.col(\"y11\"))).alias(\"z13\"))\n    .with_columns((313 * pl.col(\"y12\") / (pl.col(\"z13\") + 273)).alias(\"z12\"))\n    .select([\"a list of some 10 columns that we want to preserve\"])\n)\nframe = lazy_frame.collect()"
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html#using-pandas",
    "href": "posts/polars-vs-pandas/index.html#using-pandas",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Using pandas",
    "text": "Using pandas\nAnd here’s the pandas implementation for a total of 26 LOC. The data frames are equal up to a small difference in the way rolling is treated between polars and pandas. Overall, this implementation is much more dense. This implementation is not making use of laziness.\ndf = pd.read_parquet(path)\ndf[\"time\"] = pd.to_datetime(df[\"time\"], format=\"%m/%d/%Y %H:%M:%S.%f\")\ndf.rename(columns={\"column1\": \"criteria2\"}, inplace=True)\ndf[\"z1\"] = 16.7 * df[\"y1\"]\ndf[[\"z3\", \"z4\"]] = df.groupby([\"criteria1\", \"criteria2\"])[[\"y2\", \"z1\"]].transform(\n    lambda x: x.head(20).median()\n)[[\"y2\", \"z1\"]]\ndf[\"z5\"] = df.groupby([\"criteria1\", \"criteria2\"])[\"t\"].transform(lambda x: x.min())\ndf[\"z6\"] = df[\"z4\"] - df[\"z3\"]\ndf[\"y8\"] = df[\"z1\"] - df[\"z6\"]\ndf[\"z10\"] = df[\"y2\"].rolling(8, center=True).min() &lt; 10\ndf[\"z2\"] = df[\"y2\"].copy()\ndf.loc[\n    df[\"y2\"].rolling(8, center=True).min() &lt; 10,\n    \"z2\",\n] = float(\"nan\")\ndf[\"t\"] -= df[\"z5\"]\ndf[\"z8\"] = 2 * 3.14159 / 60 * df[\"y2\"] * df[\"y4\"]\ndf[\"z7\"] = 2 * 3.14159 / 60 * df[\"z2\"] * df[\"y4\"]\ndf[\"y9\"] = 2 * 3.14159 / 60 * df[\"y8\"] * df[\"y4\"]\ndf[\"Error\"] = df[\"y9\"] - df[\"z7\"]\ndf[\"Percentage Error\"] = 100 * df[\"Error\"] / df[\"z7\"]\ndf[\"z13\"] = 0.5 * df[\"y10\"] + 0.5 * df[\"y11\"]\ndf[\"z12\"] = 313 * df[\"y12\"] / (df[\"z13\"] + 273)\ndf = df[[\"a list of some 10 columns that we want to preserve\"]]"
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html#timing-results",
    "href": "posts/polars-vs-pandas/index.html#timing-results",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Timing results",
    "text": "Timing results\nUltimately, a promise of polars is its speed. Running both examples 10 times gives the following timing results:\npolars: 0.021s +- 0.001s\npandas: 0.181s +- 0.006s\nratios: 8.700  +- 0.375\nThat is, the polars implementation runs about 8 to 9 times faster on my 2022 MacBook Pro with an M1 Max chip and 32 GB of RAM. I am using python 3.10, pandas 1.4.3 and polars 0.16.9. Additionally, I created a larger parquet file by just concatenating the original data frame together for one hundred times, i.e.\ndf = pd.read_parquet(\"./parquet/reduced_data.parquet\")\nlargedf = pd.concat([df]*100)\nlargedf.to_parquet(\"./parquet/100reduced_data.parquet\")\nThis is of course not representing an actual dataset that is 100 times larger, but it at least shows a trend of the performance for a potentially larger dataset. Here we’re getting the following timings\npolars: 1.028s +- 0.026s\npandas: 18.813s +- 0.403s\nratios: 18.315  +- 0.490\nThe ratio between both implementations has grown when going from 40k rows to 4.4M rows. (Interestingly, the files sizes of the parquet files are 11MB and 421MB, respectively. Not an increase by 100x.)"
  },
  {
    "objectID": "posts/polars-vs-pandas/index.html#summary",
    "href": "posts/polars-vs-pandas/index.html#summary",
    "title": "Comparing polars with pandas; my personal case study",
    "section": "Summary",
    "text": "Summary\nI want to summarize two things, performance and writing code.\nPerformance wise, my polars implementation is a factor 8 to 9 faster for my example. It seems like this might be a lower bound when moving to larger files but I will keep an eye on that. Given that I am only writing polars code since a month, I am happy with this performance gain.\nFrom an implementation point of view, I was much faster using pandas. I have been using pandas for two years now and I guess that’s just showing here. I really appreciate the auto complete feature for the column names which is a feature that I miss when using polars.\nI do like the chaining of operations, but it makes the code longer and it is a bit annoying having to write code like this (using pandas):\ndf[\"y\"] = 123 + df[\"x\"]\ndf[\"z\"] = 456 + df[\"y\"]\nin this way (using polars):\ndf.with_columns((pl.col(\"x\") + 123).alias(\"y\"))\n    .with_columns((pl.col(\"y\") + 456).alias(\"z\"))\nEspecially with long column names and black formatting, this can double the number of LOC.\nOverall I am quite happy with my polars experience and I will continue using it for project in the futures. One thing I hope for, is that altair can be used with polars data frames instead of needing to call .to_pandas() when passing the data to a chart.\nThanks for reading :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CV",
    "section": "",
    "text": "Experience\nWahoo Fitness, Atlanta/remote, Algorithm Developer\nSince March 2021\n\nCreation of APIs that allow the automatic monitoring of hardware and software quality metrics.\nTime series analysis and sensor fusion for various sources of data using Python.\nStatistical analysis of sports related data and creation of interactive dashboards for visualization.\nCode generation for low-level hardware using Matlab.\nCode documentation and result summaries in Confluence.\nSprint planning using Jira in an agile environment.\n\nSteinbuch Centre for Computing, Karlsruhe, Scientific Staff\nSeptember 2017 - March 2021\n\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nUsing concepts from machine learning and optimization theory, to automatically generate near-optimal sets of problem- dependent numerical parameters for the solution of advection problems.\nI optimized the performance of research software on the HPC cluster of the Karlsruhe Institute of Technology via parameter studies and OpenMP.\nFor modules in the mathematics M.Sc. program (including Kinetic Theory and Uncertainty Quantification) and modules in the Computational Engineering Science B.Sc. program, I was a teaching assistant and substituted lectures.\n\nCenter for Computational Engineering Science, Aachen, Scientific Staff\nMarch 2015 - September 2017\n\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nHelp with lectures in the Computational Engineering Module.\nPresentations at international conferences.\n\n\n\nEducation\nKarlsruhe Institute of Technology, Ph.D. Applied Mathematics\nSeptember 2017 to March 2021\n\nDissertation: “Theory, models, and numerical methods for classical and non-classical transport”\nResearch in the field of kinetic theory, numerical mathematics, and machine learning. This encompasses the development of new algorithms to solve transport problems (e.g., the transport equation for radiation therapy), their implementation, and their analysis.\nUsing concepts from machine learning and optimization theory, to automatically generate near- optimal sets of problem-dependent numerical parameters for the solution of advection problems.\nI optimized the performance of research software on the HPC cluster of the Karlsruhe Institute of Technology via parameter studies and OpenMP.\nFor modules in the mathematics M.Sc. program (including Kinetic Theory and Uncertainty Quantification) and modules in the Computational Engineering Science B.Sc. program, I was a teaching assistant and substituted lectures.\n\nRWTH Aachen University, M.Sc. Computational Engineering Science\nSeptember 2013 to September 2015\n\nThesis: “Theory and application of numerical methods for fractional diffusion equations”\nTutoring in the field of numerical mathematics and programming.\nResearch student at the Mathematical Center for Computational Engineering Science.\nCourses on (applied) mathematics, optimization methods, and algorithmic differentiation.\n\nRWTH Aachen University, B.Sc. Computational Engineering Science\nOctober 2009 to September 2013\n\nThesis: “Improvement of the aerodynamic shape optimization by adjoint methods in an MDO process”\nInternship and Bachelor’s thesis with EADS Cassidian, Manching, Germany.\nCourses on (applied) mathematics, fluid mechanics, and high-performance computing.\n\n\n\nPublications\nJournal articles\n\nMathematische Grundlagen der künstlichen Intelligenz im Schulunterricht, Sarah Schönbrodt, TC, and Martin Frank. Mathematische Semesterberichte, pages 1–29, 2021.\nRay Effect Mitigation for the Discrete Ordinates Method Using Artificial Scattering, Martin Frank, Jonas Kusch, TC, and Cory D. Hauck. Nuclear Science and Engineering, 194(11):971–988, 2020.\nRay effect mitigation for the discrete ordinates method through quadrature rotation, TC, Martin Frank, Kerstin Küpper, and Jonas Kusch. Journal of Computational Physics, 382:105 – 123, 2019.\nA spectral galerkin method for the fractional order diffusion and wave equation, TC and Martin Frank. International Journal of Advances in Engineering Sciences and Applied Mathematics, 10(1): 90–104, 2018.\nA new high-order fluid solver for tokamak edge plasma transport simulations based on a magnetic-field independent discretization, G. Giorgiani, TC, H. Bufferand, G. Ciraolo, P. Ghendrih, H. Guillard, H. Heumann, B. Nkonga, F. Schwander, E. Serre, and P. Tamain. Contributions to Plasma Physics, 58(6-8):688–695, 2018.\n\nConference proceedings\n\nHighly uniform quadrature sets for the discrete ordinates method, TC, Martin Frank, and Jonas Kusch. In Proc. Int. Conf. Mathematics and Computational Methods Applied to Nuclear Science and Engineering, pages 25–29, 2019.\nNonclassical particle transport in heterogeneous materials, TC, Martin Frank, and Edward W Larsen. In International Conference on Mathematics & Computational Methods Applied to Nuclear Science & Engineering, 2017.\nThe equivalence of forward and backward nonclassical particle transport theories, Edward W Larsen, Martin Frank, and TC. In International Conference on Mathematics & Computational Methods Applied to Nuclear Science & Engineering, 2017.\n\nBook chapters\n\nVorschlag für eine Abiturprüfungsaufgabe mit authentischem und relevantem Realitätsbezug, Maike Sube, TC, Martin Frank, and Christina Roeckerath. Springer Berlin Heidelberg, Berlin, Heidelberg, 2020."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Data Sets",
    "section": "",
    "text": "Tour de France\nEvery cyclist and stage of the Tour de France (up to including 2022) in two CSV files.\nAlltime Athletics\nScrapes Peter Larsson’s website Alltime Athletics.\nAbiturnoten data set\nGerman Abitur: Grade distribution over time."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Title\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAbiturnoten inflation in Germany\n\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nTrack and Field World Record Progression\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nComparing polars with pandas; my personal case study\n\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\n  \n\n\n\n\nLogitech MX Mechanical Mini for Mac: Issues with M1 Mac\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nThomas Camminady\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/uncertainty/main.html",
    "href": "posts/uncertainty/main.html",
    "title": "Title",
    "section": "",
    "text": "Code\nimport polars as pl\nimport numpy as np\nimport altair as alt\n\nnp.random.seed(1)\nn = 500\ns = 10\nx = np.arange(n)\ny = np.cumsum(np.random.randn(s, n), axis=1)\n\ndf = pl.DataFrame({\"x\": x} | {str(i): y[i, :] for i in range(s)}).melt(\n    id_vars=[\"x\"],\n    value_vars=[str(i) for i in range(s)],\n    value_name=\"y\",\n    variable_name=\"sample\",\n)\n\nbase = alt.Chart(df).encode(x=\"x:Q\", y=\"y:Q\").properties(width=600, height=200)\nalt.vconcat(\n    alt.layer(\n        base.mark_line(opacity=0.3).encode(detail=\"sample:N\"),\n        base.mark_errorband(extent=\"stderr\", color=\"black\", opacity=0.5),\n    ),\n    alt.layer(\n        base.mark_line(opacity=0.3).encode(detail=\"sample:N\"),\n        base.mark_errorband(extent=\"stdev\", color=\"orange\", opacity=0.5),\n    ),\n    alt.layer(\n        base.mark_line(opacity=0.3).encode(detail=\"sample:N\"),\n        base.mark_errorband(extent=\"stdev\", color=\"orange\", opacity=0.5),\n        base.mark_errorband(extent=\"stderr\", color=\"black\", opacity=0.5),\n    ),\n)"
  }
]